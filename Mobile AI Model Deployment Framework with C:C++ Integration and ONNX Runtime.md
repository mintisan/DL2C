User: (1) è°ƒç ”å¹¶è¯„ä¼°ç§»åŠ¨ç«¯æ¨ç†æ¡†æ¶ï¼Œæ ¸å¿ƒç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½æ­£ç¡®è¿è¡Œæ¨¡å‹çš„æ–¹æ¡ˆã€‚ä¼˜å…ˆè€ƒå¯Ÿæä¾›çº¯ C è¯­è¨€æ¥å£ (C API) çš„æ¡†æ¶ï¼ˆå¦‚ ONNX Runtime C APIï¼‰ï¼Œå…¶æ¬¡æ˜¯ C++ æ¡†æ¶ï¼ˆå¦‚ PyTorch Mobile, NCNNï¼‰ã€‚é‡ç‚¹è¯„ä¼°å…¶åœ¨ ARM å¹³å°å¯¹ CNN å’Œ RNN (LSTM/GRU) ç®—å­çš„æ”¯æŒå®Œæ•´æ€§ä¸ C/C++ æ¥å£çš„æ˜“ç”¨æ€§ã€‚
(2) è®¾è®¡ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºç¡€éƒ¨ç½²å·¥ä½œæµ (Workflow)ï¼Œç¡®ä¿æ¨¡å‹èƒ½æˆåŠŸè¿è¡Œã€‚è¯¥æµç¨‹åº”ä¼˜å…ˆé€‚é…çº¯ C è¯­è¨€é›†æˆï¼Œå¹¶å…¼å®¹ C++ ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆã€‚å·¥ä½œæµéœ€æ˜ç¡®å®šä¹‰æ¨¡å‹è½¬æ¢ã€åŸºç¡€ C/C++ ä»£ç é›†æˆå’ŒåŠŸèƒ½éªŒè¯ç­‰æ ¸å¿ƒé˜¶æ®µã€‚
(3) ä»¥ ECG R-peak æ£€æµ‹æ¨¡å‹ä¸ºè¯•ç‚¹æ¡ˆä¾‹ï¼Œå®è·µå¹¶éªŒè¯è¯¥åŸºç¡€éƒ¨ç½²å·¥ä½œæµã€‚æ ¹æ®è°ƒç ”ç»“æœï¼Œä¼˜å…ˆé€‰æ‹© C è¯­è¨€æ–¹æ¡ˆè¿›è¡Œç«¯åˆ°ç«¯éƒ¨ç½²ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸ç»è¿‡ä»»ä½•åŠ é€Ÿä¼˜åŒ–çš„æƒ…å†µä¸‹èƒ½å¤Ÿæ­£ç¡®æ‰§è¡Œå¹¶è¾“å‡ºç»“æœã€‚
(4) ã€å¯é€‰æ­¥éª¤ã€‘é’ˆå¯¹å·²æˆåŠŸéƒ¨ç½²çš„æ¡†æ¶ï¼Œè°ƒç ”å…¶æ¨¡å‹åŠ é€Ÿæ–¹æ¡ˆã€‚è¿™åŒ…æ‹¬ç ”ç©¶é‡åŒ–ï¼ˆå¦‚ PTQ/QATï¼‰ã€å‰ªæç­‰ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡ C/C++ æ¥å£åˆ©ç”¨ ARM NEON ç­‰ç¡¬ä»¶åŠ é€Ÿç‰¹æ€§æ¥æå‡æ€§èƒ½ã€‚
(5) æ˜ç¡® LLM ä»£ç åŠ©æ‰‹åœ¨å·¥ä½œæµä¸­çš„è§’è‰²ã€‚ä¾‹å¦‚ï¼Œåˆ©ç”¨ LLM ç”Ÿæˆæ¨¡å‹è½¬æ¢è„šæœ¬ã€è°ƒç”¨ C/C++ API çš„åŸºç¡€æ¨ç†ä»£ç ã€ç¼–è¯‘è„šæœ¬ï¼ˆMakefile/CMakeï¼‰ï¼Œä»¥åŠåœ¨å¯é€‰çš„åŠ é€Ÿé˜¶æ®µç”Ÿæˆä¼˜åŒ–ç›¸å…³çš„ä»£ç ã€‚
(6) é’ˆå¯¹å·¥ä½œæµçš„æ¯ä¸ªç¯èŠ‚ï¼Œå¼€å‘å¯å¤ç”¨çš„ä»£ç æ¨¡æ¿å’Œè„šæœ¬ï¼Œä¼˜å…ˆåˆ›å»º C è¯­è¨€ç‰ˆæœ¬ã€‚åŒ…æ‹¬æ ‡å‡†çš„æ¨¡å‹åŠ è½½ã€é¢„å¤„ç†ã€æ¨ç†å’Œåå¤„ç†çš„å‡½æ•°æ¨¡æ¿ï¼Œå¹¶ä¸º C++ æ–¹æ¡ˆå‡†å¤‡ç›¸åº”çš„æ¨¡æ¿ï¼Œæ„æˆè‡ªåŠ¨åŒ–éƒ¨ç½²â€œAgentâ€çš„æ ¸å¿ƒã€‚
(7) ç¼–å†™è¯¦ç»†çš„æµç¨‹æ–‡æ¡£ï¼Œè®°å½•æœ€ç»ˆé€‰æ‹©çš„ C æˆ– C++ æ¨ç†åº“å’ŒæŠ€æœ¯å†³ç­–ã€‚æ–‡æ¡£éœ€æ¸…æ™°åœ°åŒºåˆ†â€œåŸºç¡€éƒ¨ç½²â€ï¼ˆå¿…é€‰ï¼‰å’Œâ€œæ€§èƒ½åŠ é€Ÿâ€ï¼ˆå¯é€‰ï¼‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¹¶æä¾›ç›¸åº”çš„æ“ä½œæŒ‡å—ã€‚
(8) åˆ›å»ºâ€œæ–°æ¨¡å‹å¿«é€Ÿè½åœ°æ¸…å•â€ï¼Œæ€»ç»“å¹¶æŒ‡å¯¼ç”¨æˆ·å¦‚ä½•åˆ©ç”¨å·²å»ºç«‹çš„å·¥ä½œæµå’Œ LLM åŠ©æ‰‹ï¼Œé¦–å…ˆå®ŒæˆåŸºç¡€éƒ¨ç½²ï¼Œç„¶åæ ¹æ®éœ€è¦é€‰æ‹©æ€§åœ°æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚

Assistant: æˆ‘å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ªå®Œæ•´çš„ç§»åŠ¨ç«¯æ¨ç†æ¡†æ¶è°ƒç ”ä¸éƒ¨ç½²å·¥ä½œæµè®¾è®¡æ–¹æ¡ˆã€‚

## 1. ç§»åŠ¨ç«¯æ¨ç†æ¡†æ¶è°ƒç ”ä¸è¯„ä¼°

### 1.1 æ¡†æ¶å¯¹æ¯”åˆ†æ

| æ¡†æ¶ | è¯­è¨€æ¥å£ | ARMæ”¯æŒ | CNNæ”¯æŒ | RNN/LSTMæ”¯æŒ | æ˜“ç”¨æ€§ | æ¨èåº¦ |
|------|----------|---------|---------|---------------|--------|--------|
| ONNX Runtime | C API + C++ | âœ… ä¼˜ç§€ | âœ… å®Œæ•´ | âœ… å®Œæ•´ | â­â­â­â­â­ | ğŸ¥‡ é¦–é€‰ |
| TensorFlow Lite | C API + C++ | âœ… ä¼˜ç§€ | âœ… å®Œæ•´ | âœ… å®Œæ•´ | â­â­â­â­ | ğŸ¥ˆ æ¨è |
| NCNN | C++ only | âœ… ä¼˜ç§€ | âœ… å®Œæ•´ | â­ æœ‰é™ | â­â­â­ | ğŸ¥‰ å¤‡é€‰ |
| PyTorch Mobile | C++ only | âœ… è‰¯å¥½ | âœ… å®Œæ•´ | âœ… å®Œæ•´ | â­â­ | å¤‡é€‰ |

### 1.2 è¯¦ç»†è¯„ä¼°æŠ¥å‘Š

**ONNX Runtime (æ¨èé¦–é€‰)**
- âœ… æä¾›çº¯C API (`onnxruntime_c_api.h`)
- âœ… ARMå¹³å°ä¼˜åŒ–è‰¯å¥½ï¼Œæ”¯æŒARM NEON
- âœ… CNNç®—å­æ”¯æŒå®Œæ•´ï¼ˆConv2D, BatchNorm, ReLUç­‰ï¼‰
- âœ… RNNç®—å­æ”¯æŒå®Œæ•´ï¼ˆLSTM, GRU, RNNï¼‰
- âœ… è·¨å¹³å°å…¼å®¹æ€§å¥½
- âœ… ç¤¾åŒºæ´»è·ƒï¼Œæ–‡æ¡£å®Œå–„

**TensorFlow Lite (æ¬¡é€‰)**
- âœ… æä¾›C API (`c_api.h`)
- âœ… ARMå¹³å°æ”¯æŒè‰¯å¥½
- âœ… ç®—å­æ”¯æŒå®Œæ•´
- âš ï¸ C APIç›¸å¯¹ç®€å•ï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€è¦C++

## 2. æ ‡å‡†åŒ–åŸºç¡€éƒ¨ç½²å·¥ä½œæµè®¾è®¡

### 2.1 å·¥ä½œæµæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ¨¡å‹å‡†å¤‡é˜¶æ®µ   â”‚ -> â”‚   è½¬æ¢é›†æˆé˜¶æ®µ   â”‚ -> â”‚   éªŒè¯éƒ¨ç½²é˜¶æ®µ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚åŸå§‹æ¨¡å‹  â”‚              â”‚ONNXæ¨¡å‹ â”‚              â”‚C/C++åº”ç”¨â”‚
    â”‚è®­ç»ƒå®Œæˆ  â”‚              â”‚C API   â”‚              â”‚æ¨ç†éªŒè¯ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚é›†æˆä»£ç  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 è¯¦ç»†å·¥ä½œæµæ­¥éª¤

**é˜¶æ®µ1: æ¨¡å‹å‡†å¤‡**
1. æ¨¡å‹æ ¼å¼æ£€æŸ¥ä¸è½¬æ¢å‡†å¤‡
2. è¾“å…¥è¾“å‡ºç»´åº¦ç¡®è®¤
3. ç®—å­å…¼å®¹æ€§éªŒè¯

**é˜¶æ®µ2: è½¬æ¢é›†æˆ**
1. æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
2. ç”ŸæˆCè¯­è¨€æ¨ç†ä»£ç 
3. ç¼–è¯‘é…ç½®ä¸ä¾èµ–ç®¡ç†

**é˜¶æ®µ3: éªŒè¯éƒ¨ç½²**
1. åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯
2. åŸºç¡€æ€§èƒ½æµ‹è¯•
3. å†…å­˜ä½¿ç”¨åˆ†æ

## 3. ECG R-peakæ£€æµ‹æ¨¡å‹è¯•ç‚¹å®è·µ

### 3.1 æ¨¡å‹è½¬æ¢è„šæœ¬

```python
# convert_ecg_model.py
import torch
import torch.onnx
import numpy as np

def convert_ecg_model_to_onnx(model_path, onnx_path):
    """å°†ECG R-peakæ£€æµ‹æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
    
    # åŠ è½½PyTorchæ¨¡å‹
    model = torch.load(model_path, map_location='cpu')
    model.eval()
    
    # å®šä¹‰ç¤ºä¾‹è¾“å…¥ (batch_size=1, channels=1, sequence_length=1000)
    dummy_input = torch.randn(1, 1, 1000)
    
    # è½¬æ¢ä¸ºONNX
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['ecg_signal'],
        output_names=['r_peaks'],
        dynamic_axes={
            'ecg_signal': {2: 'sequence_length'},
            'r_peaks': {1: 'num_peaks'}
        }
    )
    
    print(f"æ¨¡å‹å·²è½¬æ¢: {onnx_path}")

if __name__ == "__main__":
    convert_ecg_model_to_onnx("ecg_model.pth", "ecg_model.onnx")
```

### 3.2 Cè¯­è¨€æ¨ç†ä»£ç æ¨¡æ¿

```c
// ecg_inference.c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "onnxruntime_c_api.h"

typedef struct {
    const OrtApi* ort_api;
    OrtEnv* env;
    OrtSession* session;
    OrtMemoryInfo* memory_info;
} ECGInferenceContext;

// åˆå§‹åŒ–æ¨ç†ä¸Šä¸‹æ–‡
int ecg_init_inference(ECGInferenceContext* ctx, const char* model_path) {
    ctx->ort_api = OrtGetApiBase()->GetApi(ORT_API_VERSION);
    
    // åˆ›å»ºç¯å¢ƒ
    if (ctx->ort_api->CreateEnv(ORT_LOGGING_LEVEL_WARNING, "ECGInference", &ctx->env) != ORT_OK) {
        return -1;
    }
    
    // åˆ›å»ºä¼šè¯é€‰é¡¹
    OrtSessionOptions* session_options;
    ctx->ort_api->CreateSessionOptions(&session_options);
    
    // åˆ›å»ºä¼šè¯
    if (ctx->ort_api->CreateSession(ctx->env, model_path, session_options, &ctx->session) != ORT_OK) {
        return -1;
    }
    
    // åˆ›å»ºå†…å­˜ä¿¡æ¯
    ctx->ort_api->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &ctx->memory_info);
    
    ctx->ort_api->ReleaseSessionOptions(session_options);
    return 0;
}

// ECGä¿¡å·é¢„å¤„ç†
void ecg_preprocess(float* input_data, int length) {
    // å½’ä¸€åŒ–å¤„ç†
    float mean = 0.0f, std = 0.0f;
    
    // è®¡ç®—å‡å€¼
    for (int i = 0; i < length; i++) {
        mean += input_data[i];
    }
    mean /= length;
    
    // è®¡ç®—æ ‡å‡†å·®
    for (int i = 0; i < length; i++) {
        std += (input_data[i] - mean) * (input_data[i] - mean);
    }
    std = sqrt(std / length);
    
    // æ ‡å‡†åŒ–
    for (int i = 0; i < length; i++) {
        input_data[i] = (input_data[i] - mean) / std;
    }
}

// æ‰§è¡Œæ¨ç†
int ecg_run_inference(ECGInferenceContext* ctx, float* input_data, int input_length, 
                      float** output_data, int* output_length) {
    
    // é¢„å¤„ç†
    ecg_preprocess(input_data, input_length);
    
    // åˆ›å»ºè¾“å…¥tensor
    int64_t input_shape[] = {1, 1, input_length};
    OrtValue* input_tensor = NULL;
    
    ctx->ort_api->CreateTensorWithDataAsOrtValue(
        ctx->memory_info,
        input_data,
        input_length * sizeof(float),
        input_shape,
        3,
        ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,
        &input_tensor
    );
    
    // è®¾ç½®è¾“å…¥è¾“å‡ºåç§°
    const char* input_names[] = {"ecg_signal"};
    const char* output_names[] = {"r_peaks"};
    
    // è¿è¡Œæ¨ç†
    OrtValue* output_tensor = NULL;
    OrtStatus* status = ctx->ort_api->Run(
        ctx->session,
        NULL,
        input_names,
        (const OrtValue* const*)&input_tensor,
        1,
        output_names,
        1,
        &output_tensor
    );
    
    if (status != ORT_OK) {
        return -1;
    }
    
    // è·å–è¾“å‡ºæ•°æ®
    float* output_ptr;
    ctx->ort_api->GetTensorMutableData(output_tensor, (void**)&output_ptr);
    
    // è·å–è¾“å‡ºç»´åº¦
    OrtTensorTypeAndShapeInfo* output_info;
    ctx->ort_api->GetTensorTypeAndShape(output_tensor, &output_info);
    
    size_t output_count;
    ctx->ort_api->GetTensorShapeElementCount(output_info, &output_count);
    
    // åˆ†é…è¾“å‡ºå†…å­˜å¹¶å¤åˆ¶æ•°æ®
    *output_data = (float*)malloc(output_count * sizeof(float));
    memcpy(*output_data, output_ptr, output_count * sizeof(float));
    *output_length = (int)output_count;
    
    // æ¸…ç†èµ„æº
    ctx->ort_api->ReleaseValue(input_tensor);
    ctx->ort_api->ReleaseValue(output_tensor);
    ctx->ort_api->ReleaseTensorTypeAndShapeInfo(output_info);
    
    return 0;
}

// æ¸…ç†èµ„æº
void ecg_cleanup_inference(ECGInferenceContext* ctx) {
    if (ctx->session) ctx->ort_api->ReleaseSession(ctx->session);
    if (ctx->memory_info) ctx->ort_api->ReleaseMemoryInfo(ctx->memory_info);
    if (ctx->env) ctx->ort_api->ReleaseEnv(ctx->env);
}

// ä¸»å‡½æ•°ç¤ºä¾‹
int main() {
    ECGInferenceContext ctx;
    
    // åˆå§‹åŒ–
    if (ecg_init_inference(&ctx, "ecg_model.onnx") != 0) {
        printf("åˆå§‹åŒ–å¤±è´¥\n");
        return -1;
    }
    
    // æ¨¡æ‹ŸECGæ•°æ®
    int input_length = 1000;
    float* input_data = (float*)malloc(input_length * sizeof(float));
    
    // ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    for (int i = 0; i < input_length; i++) {
        input_data[i] = sin(2.0 * M_PI * i / 100.0) + 0.1 * (rand() / (float)RAND_MAX);
    }
    
    // æ‰§è¡Œæ¨ç†
    float* output_data;
    int output_length;
    
    if (ecg_run_inference(&ctx, input_data, input_length, &output_data, &output_length) == 0) {
        printf("æ¨ç†æˆåŠŸï¼Œæ£€æµ‹åˆ° %d ä¸ªRå³°\n", output_length);
        
        // æ‰“å°ç»“æœ
        for (int i = 0; i < output_length; i++) {
            printf("Rå³°ä½ç½® %d: %.2f\n", i, output_data[i]);
        }
        
        free(output_data);
    } else {
        printf("æ¨ç†å¤±è´¥\n");
    }
    
    // æ¸…ç†
    free(input_data);
    ecg_cleanup_inference(&ctx);
    
    return 0;
}
```

### 3.3 ç¼–è¯‘é…ç½® (CMakeLists.txt)

```cmake
cmake_minimum_required(VERSION 3.10)
project(ECGInference)

set(CMAKE_C_STANDARD 99)

# æŸ¥æ‰¾ONNX Runtime
find_path(ONNXRUNTIME_INCLUDE_DIR
    NAMES onnxruntime_c_api.h
    PATHS /usr/local/include/onnxruntime
          /opt/onnxruntime/include
)

find_library(ONNXRUNTIME_LIB
    NAMES onnxruntime
    PATHS /usr/local/lib
          /opt/onnxruntime/lib
)

# æ·»åŠ å¯æ‰§è¡Œæ–‡ä»¶
add_executable(ecg_inference ecg_inference.c)

# é“¾æ¥åº“
target_include_directories(ecg_inference PRIVATE ${ONNXRUNTIME_INCLUDE_DIR})
target_link_libraries(ecg_inference ${ONNXRUNTIME_LIB} m)

# ARMä¼˜åŒ–æ ‡å¿—
if(CMAKE_SYSTEM_PROCESSOR MATCHES "arm|aarch64")
    target_compile_options(ecg_inference PRIVATE -mfpu=neon -O3)
endif()
```

## 4. æ¨¡å‹åŠ é€Ÿæ–¹æ¡ˆè°ƒç ”

### 4.1 é‡åŒ–ä¼˜åŒ–

```python
# quantization.py
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

def quantize_ecg_model(model_path, quantized_path):
    """åŠ¨æ€é‡åŒ–ECGæ¨¡å‹"""
    quantize_dynamic(
        model_path,
        quantized_path,
        weight_type=QuantType.QUInt8
    )
    print(f"é‡åŒ–æ¨¡å‹ä¿å­˜è‡³: {quantized_path}")

# ä½¿ç”¨ç¤ºä¾‹
quantize_ecg_model("ecg_model.onnx", "ecg_model_quantized.onnx")
```

### 4.2 ARM NEONåŠ é€Ÿä»£ç 

```c
// neon_optimization.c
#ifdef __ARM_NEON
#include <arm_neon.h>

void vectorized_preprocess(float* data, int length) {
    // ä½¿ç”¨NEONæŒ‡ä»¤è¿›è¡Œå‘é‡åŒ–é¢„å¤„ç†
    int simd_length = length - (length % 4);
    
    for (int i = 0; i < simd_length; i += 4) {
        float32x4_t vec = vld1q_f32(&data[i]);
        // å‘é‡åŒ–æ“ä½œ
        vec = vmulq_n_f32(vec, 0.5f);  // ç¤ºä¾‹ï¼šç¼©æ”¾
        vst1q_f32(&data[i], vec);
    }
    
    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (int i = simd_length; i < length; i++) {
        data[i] *= 0.5f;
    }
}
#endif
```

## 5. LLMä»£ç åŠ©æ‰‹é›†æˆæ–¹æ¡ˆ

### 5.1 LLMåŠ©æ‰‹è§’è‰²å®šä¹‰

```python
# llm_assistant.py
class DeploymentAssistant:
    """éƒ¨ç½²åŠ©æ‰‹ç±»"""
    
    def generate_conversion_script(self, model_info):
        """ç”Ÿæˆæ¨¡å‹è½¬æ¢è„šæœ¬"""
        prompt = f"""
        è¯·ç”Ÿæˆå°†{model_info['framework']}æ¨¡å‹è½¬æ¢ä¸ºONNXçš„Pythonè„šæœ¬ã€‚
        æ¨¡å‹ç±»å‹: {model_info['type']}
        è¾“å…¥ç»´åº¦: {model_info['input_shape']}
        è¾“å‡ºç»´åº¦: {model_info['output_shape']}
        """
        return self.query_llm(prompt)
    
    def generate_inference_code(self, model_spec):
        """ç”ŸæˆCè¯­è¨€æ¨ç†ä»£ç """
        prompt = f"""
        è¯·ç”Ÿæˆä½¿ç”¨ONNX Runtime C APIçš„æ¨ç†ä»£ç ã€‚
        è¦æ±‚:
        1. åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†
        2. æ”¯æŒ{model_spec['input_type']}è¾“å…¥
        3. è¾“å‡º{model_spec['output_type']}ç»“æœ
        4. åŒ…å«é¢„å¤„ç†å’Œåå¤„ç†å‡½æ•°
        """
        return self.query_llm(prompt)
    
    def generate_build_script(self, platform_info):
        """ç”Ÿæˆç¼–è¯‘è„šæœ¬"""
        # å®ç°ç¼–è¯‘è„šæœ¬ç”Ÿæˆé€»è¾‘
        pass
```

## 6. å¯å¤ç”¨ä»£ç æ¨¡æ¿åº“

### 6.1 é€šç”¨æ¨ç†æ¨¡æ¿ (Cè¯­è¨€ç‰ˆæœ¬)

```c
// inference_template.h
#ifndef INFERENCE_TEMPLATE_H
#define INFERENCE_TEMPLATE_H

#include "onnxruntime_c_api.h"

typedef struct {
    const OrtApi* api;
    OrtEnv* env;
    OrtSession* session;
    OrtMemoryInfo* memory_info;
    char** input_names;
    char** output_names;
    int num_inputs;
    int num_outputs;
} InferenceContext;

// é€šç”¨æ¥å£
int init_inference_context(InferenceContext* ctx, const char* model_path);
int run_inference(InferenceContext* ctx, void** inputs, void** outputs);
void cleanup_inference_context(InferenceContext* ctx);

// é¢„å¤„ç†å‡½æ•°æ¨¡æ¿
void normalize_input(float* data, int length);
void resize_input(float* input, float* output, int in_w, int in_h, int out_w, int out_h);

// åå¤„ç†å‡½æ•°æ¨¡æ¿
void apply_softmax(float* data, int length);
int find_max_index(float* data, int length);

#endif
```

### 6.2 æ¨¡å‹ç‰¹åŒ–æ¨¡æ¿ç”Ÿæˆå™¨

```python
# template_generator.py
class TemplateGenerator:
    """ä»£ç æ¨¡æ¿ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.c_template = """
// {model_name}_inference.c
#include "inference_template.h"

typedef struct {{
    InferenceContext base;
    // æ¨¡å‹ç‰¹å®šé…ç½®
    {model_specific_config}
}} {ModelName}Context;

int {model_name}_init({ModelName}Context* ctx, const char* model_path) {{
    // åˆå§‹åŒ–ä»£ç 
    {init_code}
}}

int {model_name}_preprocess(float* input, int input_size, float** processed, int* processed_size) {{
    // é¢„å¤„ç†ä»£ç 
    {preprocess_code}
}}

int {model_name}_inference({ModelName}Context* ctx, float* input, float** output) {{
    // æ¨ç†ä»£ç 
    {inference_code}
}}

int {model_name}_postprocess(float* raw_output, int output_size, {output_type}* result) {{
    // åå¤„ç†ä»£ç 
    {postprocess_code}
}}
"""
    
    def generate_model_code(self, model_config):
        """æ ¹æ®é…ç½®ç”Ÿæˆç‰¹åŒ–ä»£ç """
        return self.c_template.format(**model_config)
```

## 7. è¯¦ç»†æµç¨‹æ–‡æ¡£

### 7.1 æŠ€æœ¯å†³ç­–æ–‡æ¡£

**é€‰æ‹©ONNX Runtimeçš„åŸå› :**
1. âœ… æä¾›å®Œæ•´çš„C APIï¼Œç¬¦åˆçº¯Cè¯­è¨€è¦æ±‚
2. âœ… ARMå¹³å°ä¼˜åŒ–æˆç†Ÿï¼Œæ€§èƒ½è¡¨ç°ä¼˜å¼‚
3. âœ… ç®—å­æ”¯æŒæœ€å…¨é¢ï¼Œç‰¹åˆ«æ˜¯RNNç³»åˆ—
4. âœ… ç¤¾åŒºæ´»è·ƒï¼Œé•¿æœŸç»´æŠ¤æœ‰ä¿éšœ
5. âœ… è·¨å¹³å°å…¼å®¹æ€§æœ€ä½³

**éƒ¨ç½²æµç¨‹åˆ†å±‚:**

**åŸºç¡€éƒ¨ç½²å±‚ (å¿…é€‰)**
- æ¨¡å‹è½¬æ¢ä¸éªŒè¯
- C APIé›†æˆä¸ç¼–è¯‘
- åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯

**æ€§èƒ½ä¼˜åŒ–å±‚ (å¯é€‰)**
- æ¨¡å‹é‡åŒ–ä¸å‹ç¼©
- ç¡¬ä»¶åŠ é€Ÿåˆ©ç”¨
- å†…å­˜ä¸è®¡ç®—ä¼˜åŒ–

### 7.2 æ“ä½œæŒ‡å—

**åŸºç¡€éƒ¨ç½²æ­¥éª¤:**

1. **ç¯å¢ƒå‡†å¤‡**
   ```bash
   # å®‰è£…ONNX Runtime
   wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-aarch64-1.16.0.tgz
   tar -xzf onnxruntime-linux-aarch64-1.16.0.tgz
   export ONNXRUNTIME_ROOT=/path/to/onnxruntime
   ```

2. **æ¨¡å‹è½¬æ¢**
   ```bash
   python convert_model.py --input model.pth --output model.onnx
   ```

3. **ä»£ç ç”Ÿæˆ**
   ```bash
   python generate_inference_code.py --model model.onnx --output inference.c
   ```

4. **ç¼–è¯‘éƒ¨ç½²**
   ```bash
   mkdir build && cd build
   cmake ..
   make
   ./model_inference
   ```

**æ€§èƒ½ä¼˜åŒ–æ­¥éª¤:**

1. **æ¨¡å‹é‡åŒ–**
   ```bash
   python quantize_model.py --input model.onnx --output model_quantized.onnx
   ```

2. **ç¼–è¯‘ä¼˜åŒ–**
   ```bash
   cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_NEON=ON ..
   make
   ```

## 8. æ–°æ¨¡å‹å¿«é€Ÿè½åœ°æ¸…å•

### 8.1 å¿«é€Ÿéƒ¨ç½²æ£€æŸ¥æ¸…å•

**é˜¶æ®µ1: æ¨¡å‹å‡†å¤‡ âœ“**
- [ ] ç¡®è®¤æ¨¡å‹æ ¼å¼ (PyTorch/TensorFlow/ONNX)
- [ ] éªŒè¯æ¨¡å‹è¾“å…¥è¾“å‡ºç»´åº¦
- [ ] æ£€æŸ¥ä½¿ç”¨çš„ç®—å­ç±»å‹
- [ ] å‡†å¤‡æµ‹è¯•æ•°æ®é›†

**é˜¶æ®µ2: åŸºç¡€éƒ¨ç½² âœ“**
- [ ] æ‰§è¡Œæ¨¡å‹è½¬æ¢è„šæœ¬
- [ ] ç”ŸæˆCè¯­è¨€æ¨ç†ä»£ç 
- [ ] é…ç½®ç¼–è¯‘ç¯å¢ƒ
- [ ] éªŒè¯æ¨ç†ç»“æœæ­£ç¡®æ€§
- [ ] æµ‹è¯•åŸºç¡€æ€§èƒ½æŒ‡æ ‡

**é˜¶æ®µ3: æ€§èƒ½ä¼˜åŒ– (å¯é€‰)**
- [ ] è¯„ä¼°æ˜¯å¦éœ€è¦é‡åŒ–
- [ ] æµ‹è¯•é‡åŒ–åç²¾åº¦æŸå¤±
- [ ] å¯ç”¨ç¡¬ä»¶åŠ é€Ÿé€‰é¡¹
- [ ] ä¼˜åŒ–å†…å­˜ä½¿ç”¨
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### 8.2 LLMåŠ©æ‰‹ä½¿ç”¨æŒ‡å—

**ä»£ç ç”Ÿæˆå‘½ä»¤:**
```bash
# ç”Ÿæˆè½¬æ¢è„šæœ¬
llm_assistant generate-conversion --model-type pytorch --input-shape "1,1,1000" --output-shape "1,100"

# ç”Ÿæˆæ¨ç†ä»£ç 
llm_assistant generate-inference --api onnxruntime-c --model-spec config.json

# ç”Ÿæˆç¼–è¯‘è„šæœ¬
llm_assistant generate-build --platform arm64 --optimization-level 3
```

**è‡ªåŠ¨åŒ–éƒ¨ç½²å‘½ä»¤:**
```bash
# ä¸€é”®éƒ¨ç½²
deploy_agent --model model.pth --target arm64 --framework onnxruntime --language c

# æ€§èƒ½ä¼˜åŒ–
optimize_agent --model model.onnx --methods quantization,neon --target arm64
```

### 8.3 æ•…éšœæ’é™¤æŒ‡å—

**å¸¸è§é—®é¢˜è§£å†³:**

1. **è½¬æ¢å¤±è´¥**
   - æ£€æŸ¥ç®—å­å…¼å®¹æ€§
   - æ›´æ–°ONNXç‰ˆæœ¬
   - ç®€åŒ–æ¨¡å‹ç»“æ„

2. **ç¼–è¯‘é”™è¯¯**
   - éªŒè¯åº“è·¯å¾„é…ç½®
   - æ£€æŸ¥å¤´æ–‡ä»¶åŒ…å«
   - ç¡®è®¤é“¾æ¥åº“ç‰ˆæœ¬

3. **è¿è¡Œæ—¶é”™è¯¯**
   - éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼
   - æ£€æŸ¥å†…å­˜åˆ†é…
   - ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

4. **æ€§èƒ½é—®é¢˜**
   - å¯ç”¨ç¼–è¯‘å™¨ä¼˜åŒ–
   - ä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿ
   - è€ƒè™‘æ¨¡å‹é‡åŒ–

è¿™ä¸ªå®Œæ•´çš„å·¥ä½œæµç¨‹ç¡®ä¿äº†ä»æ¨¡å‹è®­ç»ƒåˆ°ç§»åŠ¨ç«¯éƒ¨ç½²çš„æ ‡å‡†åŒ–ã€è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä¼˜å…ˆä¿è¯åŠŸèƒ½æ­£ç¡®æ€§ï¼Œç„¶åæä¾›å¯é€‰çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆã€‚é€šè¿‡LLMåŠ©æ‰‹å’Œä»£ç æ¨¡æ¿ï¼Œå¯ä»¥å¤§å¤§ç®€åŒ–æ–°æ¨¡å‹çš„éƒ¨ç½²è¿‡ç¨‹ã€‚

User: ä»¥ PyTrorch è®­ç»ƒ MNIST ä¸ºä¾‹ç»™ä¸€ä¸ªå®Œæ•´çš„è¿‡ç¨‹ã€macOSã€‘ï¼Œç»™å‡ºè®­ç»ƒ+é‡åŒ–ï¼Œå¯¼å‡ºONNXï¼ŒNetron æŸ¥çœ‹æ¨¡å‹ç»“æ„ï¼Œé‡‡ç”¨ Python ONNX æ¨ç†ï¼Œä»¥åŠ C/C++ æ¨ç†ï¼Œä»¥åŠ NDK ç¼–è¯‘æˆ ARM æ‰‹æœºå¹³å°çš„åº“æ–‡ä»¶å’Œè·¨å¹³å°å¯æ‰§è¡Œæ–‡ä»¶ï¼Œä»¥åŠé€šè¿‡ adb å¯¼å…¥æ‰‹æœºè¿è¡Œå¯¹æ¯”åŸå§‹/macOS ONNX æœ¬åœ°æ¨ç†/æ‰‹æœºæ¨ç†ç»“æœå¯¹æ¯”

Assistant: æˆ‘å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ªå®Œæ•´çš„MNISTæ¨¡å‹ä»è®­ç»ƒåˆ°ç§»åŠ¨ç«¯éƒ¨ç½²çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚

## å®Œæ•´æµç¨‹æ¦‚è§ˆ

```
è®­ç»ƒé˜¶æ®µ -> é‡åŒ–ä¼˜åŒ– -> æ¨¡å‹å¯¼å‡º -> å¯è§†åŒ–éªŒè¯ -> Pythonæ¨ç† -> C++æ¨ç† -> NDKç¼–è¯‘ -> æ‰‹æœºéƒ¨ç½² -> ç»“æœå¯¹æ¯”
```

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 macOSç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n mnist_deploy python=3.9
conda activate mnist_deploy

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio
pip install onnx onnxruntime
pip install numpy matplotlib
pip install netron

# å®‰è£…Android NDK (å¦‚æœæ²¡æœ‰)
brew install android-ndk
export ANDROID_NDK_HOME=/opt/homebrew/share/android-ndk
```

### 1.2 ç›®å½•ç»“æ„

```
mnist_project/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ quantize_model.py
â”‚   â””â”€â”€ export_onnx.py
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ python_inference.py
â”‚   â”œâ”€â”€ cpp_inference.cpp
â”‚   â””â”€â”€ android_inference.cpp
â”œâ”€â”€ build/
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â””â”€â”€ android/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mnist_model.pth
â”‚   â”œâ”€â”€ mnist_quantized.pth
â”‚   â””â”€â”€ mnist_model.onnx
â”œâ”€â”€ data/
â””â”€â”€ results/
```

## 2. PyTorchè®­ç»ƒMNISTæ¨¡å‹

### 2.1 è®­ç»ƒè„šæœ¬

```python
# train/train_model.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os

class MNISTNet(nn.Module):
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        
    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = self.conv2(x)
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return torch.log_softmax(x, dim=1)

def train_model():
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # åŠ è½½æ•°æ®
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MNISTNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.NLLLoss()
    
    # è®­ç»ƒ
    model.train()
    for epoch in range(5):  # å¿«é€Ÿè®­ç»ƒ5ä¸ªepoch
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            if batch_idx % 200 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}')
    
    # æµ‹è¯•
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'Test Accuracy: {accuracy:.2f}%')
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs('../models', exist_ok=True)
    torch.save(model.state_dict(), '../models/mnist_model.pth')
    torch.save(model, '../models/mnist_model_full.pth')
    print("æ¨¡å‹å·²ä¿å­˜åˆ° ../models/mnist_model.pth")
    
    return model

if __name__ == "__main__":
    trained_model = train_model()
```

## 3. æ¨¡å‹é‡åŒ–

### 3.1 é‡åŒ–è„šæœ¬

```python
# train/quantize_model.py
import torch
import torch.quantization as quant
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torchvision
from train_model import MNISTNet

def quantize_model():
    # åŠ è½½åŸå§‹æ¨¡å‹
    model = MNISTNet()
    model.load_state_dict(torch.load('../models/mnist_model.pth', map_location='cpu'))
    model.eval()
    
    # å‡†å¤‡æ ¡å‡†æ•°æ®
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    calibration_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=False, transform=transform
    )
    calibration_loader = DataLoader(calibration_dataset, batch_size=32, shuffle=False)
    
    # é…ç½®é‡åŒ–
    model.qconfig = quant.get_default_qconfig('fbgemm')
    
    # å‡†å¤‡é‡åŒ–
    model_prepared = quant.prepare(model, inplace=False)
    
    # æ ¡å‡†ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®ï¼‰
    model_prepared.eval()
    with torch.no_grad():
        for i, (data, _) in enumerate(calibration_loader):
            if i >= 10:  # åªä½¿ç”¨10ä¸ªbatchè¿›è¡Œæ ¡å‡†
                break
            model_prepared(data)
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    model_quantized = quant.convert(model_prepared, inplace=False)
    
    # æµ‹è¯•é‡åŒ–åçš„ç²¾åº¦
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in calibration_loader:
            outputs = model_quantized(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    accuracy = 100 * correct / total
    print(f'é‡åŒ–åæ¨¡å‹ç²¾åº¦: {accuracy:.2f}%')
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    torch.save(model_quantized.state_dict(), '../models/mnist_quantized.pth')
    torch.save(model_quantized, '../models/mnist_quantized_full.pth')
    print("é‡åŒ–æ¨¡å‹å·²ä¿å­˜")
    
    return model, model_quantized

if __name__ == "__main__":
    original, quantized = quantize_model()
```

## 4. å¯¼å‡ºONNXæ¨¡å‹

### 4.1 ONNXå¯¼å‡ºè„šæœ¬

```python
# train/export_onnx.py
import torch
import torch.onnx
import onnx
import onnxruntime
import numpy as np
from train_model import MNISTNet

def export_to_onnx():
    # åŠ è½½æ¨¡å‹
    model = MNISTNet()
    model.load_state_dict(torch.load('../models/mnist_model.pth', map_location='cpu'))
    model.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, 1, 28, 28)
    
    # å¯¼å‡ºONNX
    onnx_path = '../models/mnist_model.onnx'
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    # éªŒè¯ONNXæ¨¡å‹
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print("ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
    
    # æµ‹è¯•ONNX Runtimeæ¨ç†
    ort_session = onnxruntime.InferenceSession(onnx_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_input = dummy_input.numpy()
    
    # PyTorchæ¨ç†
    with torch.no_grad():
        pytorch_output = model(dummy_input)
    
    # ONNX Runtimeæ¨ç†
    ort_inputs = {ort_session.get_inputs()[0].name: test_input}
    ort_output = ort_session.run(None, ort_inputs)
    
    # æ¯”è¾ƒç»“æœ
    np.testing.assert_allclose(
        pytorch_output.numpy(), 
        ort_output[0], 
        rtol=1e-03, 
        atol=1e-05
    )
    print("PyTorchå’ŒONNX Runtimeæ¨ç†ç»“æœä¸€è‡´")
    print(f"ONNXæ¨¡å‹å·²å¯¼å‡ºåˆ°: {onnx_path}")
    
    return onnx_path

if __name__ == "__main__":
    onnx_path = export_to_onnx()
```

## 5. NetronæŸ¥çœ‹æ¨¡å‹ç»“æ„

### 5.1 å¯è§†åŒ–è„šæœ¬

```python
# visualize_model.py
import netron
import webbrowser
import time

def visualize_onnx_model():
    model_path = './models/mnist_model.onnx'
    
    # å¯åŠ¨NetronæœåŠ¡å™¨
    netron.start(model_path, port=8080, browse=False)
    
    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    webbrowser.open('http://localhost:8080')
    
    print("NetronæœåŠ¡å™¨å·²å¯åŠ¨: http://localhost:8080")
    print("æŒ‰Ctrl+Cåœæ­¢æœåŠ¡å™¨")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    visualize_onnx_model()
```

## 6. Python ONNXæ¨ç†

### 6.1 Pythonæ¨ç†è„šæœ¬

```python
# inference/python_inference.py
import onnxruntime as ort
import numpy as np
import torchvision
import torchvision.transforms as transforms
import torch
from PIL import Image
import time
import json

class PythonONNXInference:
    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
    def preprocess(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        if isinstance(image, Image.Image):
            transform = transforms.Compose([
                transforms.Grayscale(),
                transforms.Resize((28, 28)),
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
            tensor = transform(image)
            return tensor.unsqueeze(0).numpy()
        elif isinstance(image, np.ndarray):
            # å‡è®¾è¾“å…¥å·²ç»æ˜¯28x28çš„ç°åº¦å›¾åƒ
            image = image.astype(np.float32) / 255.0
            image = (image - 0.1307) / 0.3081
            return image.reshape(1, 1, 28, 28)
        else:
            return image
    
    def inference(self, input_data):
        """æ‰§è¡Œæ¨ç†"""
        start_time = time.time()
        
        ort_inputs = {self.input_name: input_data}
        ort_outputs = self.session.run([self.output_name], ort_inputs)
        
        inference_time = time.time() - start_time
        
        # åº”ç”¨softmaxè·å¾—æ¦‚ç‡
        logits = ort_outputs[0]
        probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
        predicted_class = np.argmax(probabilities, axis=1)[0]
        confidence = probabilities[0][predicted_class]
        
        return {
            'predicted_class': int(predicted_class),
            'confidence': float(confidence),
            'probabilities': probabilities[0].tolist(),
            'inference_time_ms': inference_time * 1000,
            'raw_logits': logits[0].tolist()
        }

def test_python_inference():
    """æµ‹è¯•Pythonæ¨ç†"""
    print("=== Python ONNXæ¨ç†æµ‹è¯• ===")
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inference_engine = PythonONNXInference('./models/mnist_model.onnx')
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=False, transform=transform
    )
    
    # æµ‹è¯•å¤šä¸ªæ ·æœ¬
    results = []
    test_samples = 10
    
    for i in range(test_samples):
        image, true_label = test_dataset[i]
        input_data = image.unsqueeze(0).numpy()
        
        result = inference_engine.inference(input_data)
        result['true_label'] = int(true_label)
        result['sample_id'] = i
        
        results.append(result)
        
        print(f"æ ·æœ¬ {i}: çœŸå®={true_label}, é¢„æµ‹={result['predicted_class']}, "
              f"ç½®ä¿¡åº¦={result['confidence']:.4f}, æ—¶é—´={result['inference_time_ms']:.2f}ms")
    
    # è®¡ç®—å‡†ç¡®ç‡
    correct = sum(1 for r in results if r['predicted_class'] == r['true_label'])
    accuracy = correct / len(results)
    avg_time = np.mean([r['inference_time_ms'] for r in results])
    
    print(f"\nå‡†ç¡®ç‡: {accuracy:.2%}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}ms")
    
    # ä¿å­˜ç»“æœ
    with open('./results/python_inference_results.json', 'w') as f:
        json.dump({
            'results': results,
            'summary': {
                'accuracy': accuracy,
                'average_inference_time_ms': avg_time,
                'total_samples': len(results)
            }
        }, f, indent=2)
    
    return results

if __name__ == "__main__":
    import os
    os.makedirs('./results', exist_ok=True)
    test_python_inference()
```

## 7. C++æ¨ç†å®ç°

### 7.1 C++æ¨ç†ä»£ç 

```cpp
// inference/cpp_inference.cpp
#include <iostream>
#include <vector>
#include <chrono>
#include <fstream>
#include <memory>
#include <algorithm>
#include <cmath>
#include <iomanip>
#include <onnxruntime_cxx_api.h>

class CPPONNXInference {
private:
    Ort::Env env;
    Ort::Session session;
    Ort::MemoryInfo memory_info;
    std::vector<const char*> input_names;
    std::vector<const char*> output_names;
    std::vector<int64_t> input_shape;
    std::vector<int64_t> output_shape;

public:
    CPPONNXInference(const std::string& model_path) 
        : env(ORT_LOGGING_LEVEL_WARNING, "CPPONNXInference"),
          session(env, model_path.c_str(), Ort::SessionOptions{}),
          memory_info(Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault)) {
        
        // è·å–è¾“å…¥ä¿¡æ¯
        size_t num_input_nodes = session.GetInputCount();
        for (size_t i = 0; i < num_input_nodes; i++) {
            auto input_name = session.GetInputNameAllocated(i, Ort::AllocatorWithDefaultOptions{});
            input_names.push_back(input_name.release());
            
            Ort::TypeInfo input_type_info = session.GetInputTypeInfo(i);
            auto input_tensor_info = input_type_info.GetTensorTypeAndShapeInfo();
            input_shape = input_tensor_info.GetShape();
        }
        
        // è·å–è¾“å‡ºä¿¡æ¯
        size_t num_output_nodes = session.GetOutputCount();
        for (size_t i = 0; i < num_output_nodes; i++) {
            auto output_name = session.GetOutputNameAllocated(i, Ort::AllocatorWithDefaultOptions{});
            output_names.push_back(output_name.release());
            
            Ort::TypeInfo output_type_info = session.GetOutputTypeInfo(i);
            auto output_tensor_info = output_type_info.GetTensorTypeAndShapeInfo();
            output_shape = output_tensor_info.GetShape();
        }
        
        std::cout << "æ¨¡å‹åŠ è½½æˆåŠŸ" << std::endl;
        std::cout << "è¾“å…¥å½¢çŠ¶: [";
        for (size_t i = 0; i < input_shape.size(); ++i) {
            std::cout << input_shape[i];
            if (i < input_shape.size() - 1) std::cout << ", ";
        }
        std::cout << "]" << std::endl;
    }
    
    ~CPPONNXInference() {
        for (auto& name : input_names) {
            delete[] name;
        }
        for (auto& name : output_names) {
            delete[] name;
        }
    }
    
    struct InferenceResult {
        int predicted_class;
        float confidence;
        std::vector<float> probabilities;
        double inference_time_ms;
        std::vector<float> raw_logits;
    };
    
    std::vector<float> preprocess(const std::vector<float>& raw_data) {
        std::vector<float> processed = raw_data;
        
        // å½’ä¸€åŒ– (å‡è®¾è¾“å…¥å·²ç»æ˜¯0-1èŒƒå›´)
        const float mean = 0.1307f;
        const float std = 0.3081f;
        
        for (auto& pixel : processed) {
            pixel = (pixel - mean) / std;
        }
        
        return processed;
    }
    
    std::vector<float> softmax(const std::vector<float>& logits) {
        std::vector<float> probabilities(logits.size());
        float max_logit = *std::max_element(logits.begin(), logits.end());
        
        float sum = 0.0f;
        for (size_t i = 0; i < logits.size(); ++i) {
            probabilities[i] = std::exp(logits[i] - max_logit);
            sum += probabilities[i];
        }
        
        for (auto& prob : probabilities) {
            prob /= sum;
        }
        
        return probabilities;
    }
    
    InferenceResult inference(const std::vector<float>& input_data) {
        auto start = std::chrono::high_resolution_clock::now();
        
        // é¢„å¤„ç†
        auto processed_data = preprocess(input_data);
        
        // åˆ›å»ºè¾“å…¥tensor
        auto input_tensor = Ort::Value::CreateTensor<float>(
            memory_info, 
            const_cast<float*>(processed_data.data()), 
            processed_data.size(),
            input_shape.data(), 
            input_shape.size()
        );
        
        // è¿è¡Œæ¨ç†
        auto output_tensors = session.Run(
            Ort::RunOptions{nullptr}, 
            input_names.data(), 
            &input_tensor, 
            1, 
            output_names.data(), 
            1
        );
        
        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
        
        // è·å–è¾“å‡ºæ•°æ®
        float* output_data = output_tensors.front().GetTensorMutableData<float>();
        size_t output_size = output_tensors.front().GetTensorTypeAndShapeInfo().GetElementCount();
        
        std::vector<float> logits(output_data, output_data + output_size);
        std::vector<float> probabilities = softmax(logits);
        
        // æ‰¾åˆ°é¢„æµ‹ç±»åˆ«
        auto max_it = std::max_element(probabilities.begin(), probabilities.end());
        int predicted_class = std::distance(probabilities.begin(), max_it);
        float confidence = *max_it;
        
        InferenceResult result;
        result.predicted_class = predicted_class;
        result.confidence = confidence;
        result.probabilities = probabilities;
        result.inference_time_ms = duration.count() / 1000.0;
        result.raw_logits = logits;
        
        return result;
    }
};

// è¯»å–MNISTæµ‹è¯•æ•°æ®çš„ç®€å•å®ç°
std::vector<std::pair<std::vector<float>, int>> load_mnist_test_data(const std::string& data_path, int num_samples = 10) {
    // è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥è¯»å–MNISTäºŒè¿›åˆ¶æ–‡ä»¶
    // ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›éšæœºæ•°æ®
    std::vector<std::pair<std::vector<float>, int>> data;
    
    for (int i = 0; i < num_samples; ++i) {
        std::vector<float> image(28 * 28);
        for (int j = 0; j < 28 * 28; ++j) {
            image[j] = static_cast<float>(rand()) / RAND_MAX;
        }
        int label = rand() % 10;
        data.emplace_back(image, label);
    }
    
    return data;
}

int main() {
    try {
        std::cout << "=== C++ ONNXæ¨ç†æµ‹è¯• ===" << std::endl;
        
        // åˆå§‹åŒ–æ¨ç†å¼•æ“
        CPPONNXInference inference_engine("../models/mnist_model.onnx");
        
        // åŠ è½½æµ‹è¯•æ•°æ®
        auto test_data = load_mnist_test_data("../data", 10);
        
        std::vector<CPPONNXInference::InferenceResult> results;
        int correct = 0;
        
        for (size_t i = 0; i < test_data.size(); ++i) {
            const auto& [image, true_label] = test_data[i];
            
            auto result = inference_engine.inference(image);
            results.push_back(result);
            
            if (result.predicted_class == true_label) {
                correct++;
            }
            
            std::cout << "æ ·æœ¬ " << i << ": çœŸå®=" << true_label 
                      << ", é¢„æµ‹=" << result.predicted_class
                      << ", ç½®ä¿¡åº¦=" << std::fixed << std::setprecision(4) << result.confidence
                      << ", æ—¶é—´=" << std::setprecision(2) << result.inference_time_ms << "ms"
                      << std::endl;
        }
        
        double accuracy = static_cast<double>(correct) / test_data.size();
        double avg_time = 0.0;
        for (const auto& result : results) {
            avg_time += result.inference_time_ms;
        }
        avg_time /= results.size();
        
        std::cout << std::endl;
        std::cout << "å‡†ç¡®ç‡: " << std::setprecision(2) << accuracy * 100 << "%" << std::endl;
        std::cout << "å¹³å‡æ¨ç†æ—¶é—´: " << std::setprecision(2) << avg_time << "ms" << std::endl;
        
        // ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        std::ofstream result_file("../results/cpp_inference_results.json");
        result_file << "{\n";
        result_file << "  \"summary\": {\n";
        result_file << "    \"accuracy\": " << accuracy << ",\n";
        result_file << "    \"average_inference_time_ms\": " << avg_time << ",\n";
        result_file << "    \"total_samples\": " << results.size() << "\n";
        result_file << "  }\n";
        result_file << "}\n";
        result_file.close();
        
    } catch (const std::exception& e) {
        std::cerr << "é”™è¯¯: " << e.what() << std::endl;
        return -1;
    }
    
    return 0;
}
```

### 7.2 C++ç¼–è¯‘é…ç½®

```cmake
# build/CMakeLists.txt
cmake_minimum_required(VERSION 3.16)
project(MNISTInference)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# æŸ¥æ‰¾ONNX Runtime
if(APPLE)
    # macOSè·¯å¾„
    set(ONNXRUNTIME_ROOT_PATH "/usr/local/onnxruntime")
    if(NOT EXISTS ${ONNXRUNTIME_ROOT_PATH})
        # å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬
        set(ONNXRUNTIME_ROOT_PATH "${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-osx-x86_64-1.16.0")
    endif()
elseif(ANDROID)
    # Androidè·¯å¾„
    set(ONNXRUNTIME_ROOT_PATH "${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-android-${ANDROID_ABI}")
endif()

set(ONNXRUNTIME_INCLUDE_DIRS ${ONNXRUNTIME_ROOT_PATH}/include)
set(ONNXRUNTIME_LIB ${ONNXRUNTIME_ROOT_PATH}/lib/libonnxruntime.dylib)

if(ANDROID)
    set(ONNXRUNTIME_LIB ${ONNXRUNTIME_ROOT_PATH}/lib/libonnxruntime.so)
endif()

# æ·»åŠ å¯æ‰§è¡Œæ–‡ä»¶
add_executable(mnist_inference_cpp ../inference/cpp_inference.cpp)

# è®¾ç½®åŒ…å«ç›®å½•
target_include_directories(mnist_inference_cpp PRIVATE ${ONNXRUNTIME_INCLUDE_DIRS})

# é“¾æ¥åº“
target_link_libraries(mnist_inference_cpp ${ONNXRUNTIME_LIB})

# Androidç‰¹å®šé…ç½®
if(ANDROID)
    # åˆ›å»ºå…±äº«åº“ç‰ˆæœ¬
    add_library(mnist_inference_lib SHARED ../inference/android_inference.cpp)
    target_include_directories(mnist_inference_lib PRIVATE ${ONNXRUNTIME_INCLUDE_DIRS})
    target_link_libraries(mnist_inference_lib ${ONNXRUNTIME_LIB} log)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    set_target_properties(mnist_inference_lib PROPERTIES
        LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_SOURCE_DIR}/../android_libs/${ANDROID_ABI}"
    )
endif()

# ç¼–è¯‘ä¼˜åŒ–
if(CMAKE_BUILD_TYPE STREQUAL "Release")
    if(ANDROID AND CMAKE_ANDROID_ARCH_ABI MATCHES "arm")
        target_compile_options(mnist_inference_cpp PRIVATE -mfpu=neon -O3)
        if(TARGET mnist_inference_lib)
            target_compile_options(mnist_inference_lib PRIVATE -mfpu=neon -O3)
        endif()
    endif()
endif()
```

## 8. Android NDKç¼–è¯‘

### 8.1 Androidä¸“ç”¨æ¨ç†ä»£ç 

```cpp
// inference/android_inference.cpp
#include <jni.h>
#include <android/log.h>
#include <string>
#include <vector>
#include <memory>
#include <onnxruntime_cxx_api.h>

#define LOG_TAG "MNISTInference"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)

class AndroidMNISTInference {
private:
    std::unique_ptr<Ort::Env> env;
    std::unique_ptr<Ort::Session> session;
    std::unique_ptr<Ort::MemoryInfo> memory_info;
    std::vector<const char*> input_names;
    std::vector<const char*> output_names;
    std::vector<int64_t> input_shape;

public:
    AndroidMNISTInference() {
        env = std::make_unique<Ort::Env>(ORT_LOGGING_LEVEL_WARNING, "AndroidMNISTInference");
        memory_info = std::make_unique<Ort::MemoryInfo>(
            Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault)
        );
    }
    
    bool loadModel(const std::string& model_path) {
        try {
            Ort::SessionOptions session_options;
            session_options.SetIntraOpNumThreads(1);
            session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
            
            session = std::make_unique<Ort::Session>(*env, model_path.c_str(), session_options);
            
            // è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            size_t num_input_nodes = session->GetInputCount();
            for (size_t i = 0; i < num_input_nodes; i++) {
                auto input_name = session->GetInputNameAllocated(i, Ort::AllocatorWithDefaultOptions{});
                input_names.push_back(input_name.release());
                
                Ort::TypeInfo input_type_info = session->GetInputTypeInfo(i);
                auto input_tensor_info = input_type_info.GetTensorTypeAndShapeInfo();
                input_shape = input_tensor_info.GetShape();
            }
            
            size_t num_output_nodes = session->GetOutputCount();
            for (size_t i = 0; i < num_output_nodes; i++) {
                auto output_name = session->GetOutputNameAllocated(i, Ort::AllocatorWithDefaultOptions{});
                output_names.push_back(output_name.release());
            }
            
            LOGI("æ¨¡å‹åŠ è½½æˆåŠŸ");
            return true;
        } catch (const std::exception& e) {
            LOGE("æ¨¡å‹åŠ è½½å¤±è´¥: %s", e.what());
            return false;
        }
    }
    
    std::pair<int, float> inference(const std::vector<float>& input_data) {
        try {
            // é¢„å¤„ç†
            std::vector<float> processed_data = input_data;
            const float mean = 0.1307f;
            const float std = 0.3081f;
            
            for (auto& pixel : processed_data) {
                pixel = (pixel - mean) / std;
            }
            
            // åˆ›å»ºè¾“å…¥tensor
            auto input_tensor = Ort::Value::CreateTensor<float>(
                *memory_info,
                processed_data.data(),
                processed_data.size(),
                input_shape.data(),
                input_shape.size()
            );
            
            // è¿è¡Œæ¨ç†
            auto output_tensors = session->Run(
                Ort::RunOptions{nullptr},
                input_names.data(),
                &input_tensor,
                1,
                output_names.data(),
                1
            );
            
            // è·å–è¾“å‡º
            float* output_data = output_tensors.front().GetTensorMutableData<float>();
            size_t output_size = output_tensors.front().GetTensorTypeAndShapeInfo().GetElementCount();
            
            // åº”ç”¨softmax
            std::vector<float> probabilities(output_size);
            float max_logit = *std::max_element(output_data, output_data + output_size);
            float sum = 0.0f;
            
            for (size_t i = 0; i < output_size; ++i) {
                probabilities[i] = std::exp(output_data[i] - max_logit);
                sum += probabilities[i];
            }
            
            for (auto& prob : probabilities) {
                prob /= sum;
            }
            
            // æ‰¾åˆ°é¢„æµ‹ç±»åˆ«
            auto max_it = std::max_element(probabilities.begin(), probabilities.end());
            int predicted_class = std::distance(probabilities.begin(), max_it);
            float confidence = *max_it;
            
            return {predicted_class, confidence};
            
        } catch (const std::exception& e) {
            LOGE("æ¨ç†å¤±è´¥: %s", e.what());
            return {-1, 0.0f};
        }
    }
};

// å…¨å±€æ¨ç†å¼•æ“å®ä¾‹
static std::unique_ptr<AndroidMNISTInference> g_inference_engine;

extern "C" {

JNIEXPORT jboolean JNICALL
Java_com_example_mnistinference_MNISTInference_loadModel(JNIEnv *env, jobject thiz, jstring model_path) {
    const char* path = env->GetStringUTFChars(model_path, nullptr);
    
    g_inference_engine = std::make_unique<AndroidMNISTInference>();
    bool success = g_inference_engine->loadModel(std::string(path));
    
    env->ReleaseStringUTFChars(model_path, path);
    return success;
}

JNIEXPORT jfloatArray JNICALL
Java_com_example_mnistinference_MNISTInference_inference(JNIEnv *env, jobject thiz, jfloatArray input_data) {
    if (!g_inference_engine) {
        LOGE("æ¨ç†å¼•æ“æœªåˆå§‹åŒ–");
        return nullptr;
    }
    
    // è·å–è¾“å…¥æ•°æ®
    jsize input_size = env->GetArrayLength(input_data);
    jfloat* input_ptr = env->GetFloatArrayElements(input_data, nullptr);
    
    std::vector<float> input_vector(input_ptr, input_ptr + input_size);
    
    // æ‰§è¡Œæ¨ç†
    auto [predicted_class, confidence] = g_inference_engine->inference(input_vector);
    
    // åˆ›å»ºè¿”å›æ•°ç»„ [predicted_class, confidence]
    jfloatArray result = env->NewFloatArray(2);
    jfloat result_data[2] = {static_cast<jfloat>(predicted_class), confidence};
    env->SetFloatArrayRegion(result, 0, 2, result_data);
    
    env->ReleaseFloatArrayElements(input_data, input_ptr, JNI_ABORT);
    
    return result;
}

} // extern "C"
```

### 8.2 ç¼–è¯‘è„šæœ¬

```bash
#!/bin/bash
# build/build_android.sh

set -e

# è®¾ç½®å˜é‡
ANDROID_NDK_HOME=${ANDROID_NDK_HOME:-"/opt/homebrew/share/android-ndk"}
ANDROID_API=21
PROJECT_DIR=$(pwd)/..

# ä¸‹è½½ONNX Runtime for Android
download_onnxruntime() {
    local abi=$1
    local version="1.16.0"
    local url="https://github.com/microsoft/onnxruntime/releases/download/v${version}/onnxruntime-android-${abi}-${version}.tgz"
    local target_dir="onnxruntime-android-${abi}"
    
    if [ ! -d "${target_dir}" ]; then
        echo "ä¸‹è½½ ONNX Runtime for ${abi}..."
        wget -O "${target_dir}.tgz" "${url}"
        tar -xzf "${target_dir}.tgz"
        rm "${target_dir}.tgz"
    fi
}

# ç¼–è¯‘å‡½æ•°
build_for_abi() {
    local abi=$1
    echo "ç¼–è¯‘ ${abi} ç‰ˆæœ¬..."
    
    # ä¸‹è½½å¯¹åº”çš„ONNX Runtime
    download_onnxruntime ${abi}
    
    # åˆ›å»ºæ„å»ºç›®å½•
    mkdir -p "build_${abi}"
    cd "build_${abi}"
    
    # é…ç½®CMake
    cmake \
        -DCMAKE_TOOLCHAIN_FILE="${ANDROID_NDK_HOME}/build/cmake/android.toolchain.cmake" \
        -DANDROID_ABI=${abi} \
        -DANDROID_NATIVE_API_LEVEL=${ANDROID_API} \
        -DCMAKE_BUILD_TYPE=Release \
        -DANDROID_STL=c++_shared \
        ..
    
    # ç¼–è¯‘
    make -j$(nproc)
    
    cd ..
}

# ä¸»å‡½æ•°
main() {
    echo "å¼€å§‹ç¼–è¯‘Androidç‰ˆæœ¬..."
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "${PROJECT_DIR}/android_libs"
    
    # ç¼–è¯‘ä¸åŒæ¶æ„
    for abi in "arm64-v8a" "armeabi-v7a" "x86" "x86_64"; do
        build_for_abi ${abi}
    done
    
    echo "ç¼–è¯‘å®Œæˆï¼"
    echo "åº“æ–‡ä»¶ä½ç½®: ${PROJECT_DIR}/android_libs/"
}

# æ‰§è¡Œä¸»å‡½æ•°
main
```

### 8.3 å¯æ‰§è¡Œæ–‡ä»¶ç¼–è¯‘

```bash
#!/bin/bash
# build/build_executable.sh

# ç¼–è¯‘macOSç‰ˆæœ¬
build_macos() {
    echo "ç¼–è¯‘macOSç‰ˆæœ¬..."
    
    # ä¸‹è½½ONNX Runtime for macOS
    if [ ! -d "onnxruntime-osx-x86_64-1.16.0" ]; then
        wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-osx-x86_64-1.16.0.tgz
        tar -xzf onnxruntime-osx-x86_64-1.16.0.tgz
        rm onnxruntime-osx-x86_64-1.16.0.tgz
    fi
    
    mkdir -p build_macos
    cd build_macos
    
    cmake -DCMAKE_BUILD_TYPE=Release ..
    make -j$(sysctl -n hw.ncpu)
    
    cd ..
}

# ç¼–è¯‘Androidå¯æ‰§è¡Œæ–‡ä»¶
build_android_executable() {
    local abi=$1
    echo "ç¼–è¯‘Androidå¯æ‰§è¡Œæ–‡ä»¶ (${abi})..."
    
    mkdir -p "build_android_exe_${abi}"
    cd "build_android_exe_${abi}"
    
    # åˆ›å»ºç®€å•çš„CMakeListsç”¨äºå¯æ‰§è¡Œæ–‡ä»¶
    cat > CMakeLists.txt << EOF
cmake_minimum_required(VERSION 3.16)
project(MNISTInferenceExecutable)

set(CMAKE_CXX_STANDARD 17)

set(ONNXRUNTIME_ROOT_PATH "\${CMAKE_CURRENT_SOURCE_DIR}/../onnxruntime-android-${abi}")
set(ONNXRUNTIME_INCLUDE_DIRS \${ONNXRUNTIME_ROOT_PATH}/include)
set(ONNXRUNTIME_LIB \${ONNXRUNTIME_ROOT_PATH}/lib/libonnxruntime.so)

add_executable(mnist_inference ../../inference/cpp_inference.cpp)
target_include_directories(mnist_inference PRIVATE \${ONNXRUNTIME_INCLUDE_DIRS})
target_link_libraries(mnist_inference \${ONNXRUNTIME_LIB})
EOF
    
    cmake \
        -DCMAKE_TOOLCHAIN_FILE="${ANDROID_NDK_HOME}/build/cmake/android.toolchain.cmake" \
        -DANDROID_ABI=${abi} \
        -DANDROID_NATIVE_API_LEVEL=21 \
        -DCMAKE_BUILD_TYPE=Release \
        -DANDROID_STL=c++_shared \
        .
    
    make -j$(nproc)
    
    # å¤åˆ¶å¯æ‰§è¡Œæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    mkdir -p "../android_executables/${abi}"
    cp mnist_inference "../android_executables/${abi}/"
    cp "../onnxruntime-android-${abi}/lib/libonnxruntime.so" "../android_executables/${abi}/"
    
    cd ..
}

# ä¸»å‡½æ•°
main() {
    # ç¼–è¯‘macOSç‰ˆæœ¬
    build_macos
    
    # ç¼–è¯‘Androidå¯æ‰§è¡Œæ–‡ä»¶
    mkdir -p android_executables
    for abi in "arm64-v8a" "armeabi-v7a"; do
        build_android_executable ${abi}
    done
    
    echo "æ‰€æœ‰ç‰ˆæœ¬ç¼–è¯‘å®Œæˆï¼"
}

main
```

## 9. æ‰‹æœºéƒ¨ç½²ä¸æµ‹è¯•

### 9.1 éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy_and_test.sh

set -e

PROJECT_DIR=$(pwd)
DEVICE_DIR="/data/local/tmp/mnist_inference"

# æ£€æŸ¥è®¾å¤‡è¿æ¥
check_device() {
    if ! adb devices | grep -q "device$"; then
        echo "é”™è¯¯: æ²¡æœ‰æ£€æµ‹åˆ°Androidè®¾å¤‡"
        echo "è¯·ç¡®ä¿è®¾å¤‡å·²è¿æ¥å¹¶å¼€å¯USBè°ƒè¯•"
        exit 1
    fi
    echo "è®¾å¤‡è¿æ¥æ­£å¸¸"
}

# è·å–è®¾å¤‡æ¶æ„
get_device_arch() {
    local arch=$(adb shell getprop ro.product.cpu.abi | tr -d '\r')
    echo $arch
}

# éƒ¨ç½²æ–‡ä»¶åˆ°è®¾å¤‡
deploy_to_device() {
    local arch=$1
    
    echo "éƒ¨ç½²åˆ°è®¾å¤‡ (${arch})..."
    
    # åˆ›å»ºè®¾å¤‡ç›®å½•
    adb shell "mkdir -p ${DEVICE_DIR}"
    
    # æ¨é€å¯æ‰§è¡Œæ–‡ä»¶å’Œåº“
    adb push "android_executables/${arch}/mnist_inference" "${DEVICE_DIR}/"
    adb push "android_executables/${arch}/libonnxruntime.so" "${DEVICE_DIR}/"
    adb push "models/mnist_model.onnx" "${DEVICE_DIR}/"
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    adb shell "chmod +x ${DEVICE_DIR}/mnist_inference"
    
    echo "æ–‡ä»¶éƒ¨ç½²å®Œæˆ"
}

# ç”Ÿæˆæµ‹è¯•æ•°æ®
generate_test_data() {
    cat > generate_test_data.py << 'EOF'
import numpy as np
import json
import torchvision
import torchvision.transforms as transforms

# åŠ è½½MNISTæµ‹è¯•æ•°æ®
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

test_dataset = torchvision.datasets.MNIST(
    root='./data', train=False, download=False, transform=transform
)

# ç”Ÿæˆæµ‹è¯•æ ·æœ¬
test_samples = []
for i in range(10):
    image, label = test_dataset[i]
    # è½¬æ¢ä¸ºåŸå§‹åƒç´ å€¼ (0-1èŒƒå›´ï¼Œæœªå½’ä¸€åŒ–)
    raw_image = (image * 0.3081 + 0.1307).clamp(0, 1)
    test_samples.append({
        'id': i,
        'label': int(label),
        'data': raw_image.flatten().tolist()
    })

# ä¿å­˜æµ‹è¯•æ•°æ®
with open('test_data.json', 'w') as f:
    json.dump(test_samples, f, indent=2)

print("æµ‹è¯•æ•°æ®å·²ç”Ÿæˆ: test_data.json")
EOF

    python generate_test_data.py
    adb push test_data.json "${DEVICE_DIR}/"
}

# åœ¨è®¾å¤‡ä¸Šè¿è¡Œæµ‹è¯•
run_device_test() {
    echo "åœ¨è®¾å¤‡ä¸Šè¿è¡Œæ¨ç†æµ‹è¯•..."
    
    # è®¾ç½®åº“è·¯å¾„å¹¶è¿è¡Œ
    adb shell "cd ${DEVICE_DIR} && LD_LIBRARY_PATH=. ./mnist_inference > device_results.txt 2>&1"
    
    # è·å–ç»“æœ
    adb pull "${DEVICE_DIR}/device_results.txt" "./results/"
    
    echo "è®¾å¤‡æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ° ./results/device_results.txt"
}

# ç»“æœå¯¹æ¯”
compare_results() {
    echo "=== ç»“æœå¯¹æ¯”åˆ†æ ==="
    
    cat > compare_results.py << 'EOF'
import json
import numpy as np

def load_results(file_path):
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except:
        return None

def parse_device_results(file_path):
    """è§£æè®¾å¤‡è¾“å‡ºçš„æ–‡æœ¬ç»“æœ"""
    results = []
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                if "æ ·æœ¬" in line and "é¢„æµ‹=" in line:
                    # è§£æè¡Œ: "æ ·æœ¬ 0: çœŸå®=7, é¢„æµ‹=7, ç½®ä¿¡åº¦=0.9999, æ—¶é—´=1.23ms"
                    parts = line.strip().split(', ')
                    sample_info = parts[0].split(': ')
                    sample_id = int(sample_info[0].split(' ')[1])
                    true_label = int(sample_info[1].split('=')[1])
                    predicted = int(parts[1].split('=')[1])
                    confidence = float(parts[2].split('=')[1])
                    time_ms = float(parts[3].split('=')[1].replace('ms', ''))
                    
                    results.append({
                        'sample_id': sample_id,
                        'true_label': true_label,
                        'predicted_class': predicted,
                        'confidence': confidence,
                        'inference_time_ms': time_ms
                    })
    except Exception as e:
        print(f"è§£æè®¾å¤‡ç»“æœå¤±è´¥: {e}")
    
    return results

# åŠ è½½ç»“æœ
python_results = load_results('./results/python_inference_results.json')
cpp_results = load_results('./results/cpp_inference_results.json')
device_results = parse_device_results('./results/device_results.txt')

print("=== æ¨ç†ç»“æœå¯¹æ¯” ===")
print(f"Python ONNX: {len(python_results['results']) if python_results else 0} ä¸ªæ ·æœ¬")
print(f"C++ ONNX:    {len(cpp_results) if cpp_results else 0} ä¸ªæ ·æœ¬") 
print(f"è®¾å¤‡æ¨ç†:    {len(device_results)} ä¸ªæ ·æœ¬")

if python_results and device_results:
    print("\n=== å‡†ç¡®ç‡å¯¹æ¯” ===")
    
    # Pythonç»“æœ
    py_correct = sum(1 for r in python_results['results'] if r['predicted_class'] == r['true_label'])
    py_accuracy = py_correct / len(python_results['results'])
    py_avg_time = python_results['summary']['average_inference_time_ms']
    
    # è®¾å¤‡ç»“æœ
    dev_correct = sum(1 for r in device_results if r['predicted_class'] == r['true_label'])
    dev_accuracy = dev_correct / len(device_results) if device_results else 0
    dev_avg_time = np.mean([r['inference_time_ms'] for r in device_results]) if device_results else 0
    
    print(f"Python (macOS): {py_accuracy:.2%}, å¹³å‡æ—¶é—´: {py_avg_time:.2f}ms")
    print(f"è®¾å¤‡ (Android): {dev_accuracy:.2%}, å¹³å‡æ—¶é—´: {dev_avg_time:.2f}ms")
    
    print("\n=== é€æ ·æœ¬å¯¹æ¯” ===")
    for i in range(min(len(python_results['results']), len(device_results))):
        py_r = python_results['results'][i]
        dev_r = device_results[i]
        
        match = "âœ“" if py_r['predicted_class'] == dev_r['predicted_class'] else "âœ—"
        print(f"æ ·æœ¬{i}: macOS={py_r['predicted_class']}({py_r['confidence']:.3f}), "
              f"Android={dev_r['predicted_class']}({dev_r['confidence']:.3f}) {match}")

print("\n=== æ€§èƒ½å¯¹æ¯” ===")
if python_results and device_results:
    speedup = py_avg_time / dev_avg_time if dev_avg_time > 0 else 0
    print(f"é€Ÿåº¦æå‡: {speedup:.2f}x" if speedup > 1 else f"é€Ÿåº¦ä¸‹é™: {1/speedup:.2f}x")

EOF

    python compare_results.py
}

# ä¸»å‡½æ•°
main() {
    echo "=== MNISTæ¨¡å‹æ‰‹æœºéƒ¨ç½²æµ‹è¯• ==="
    
    # æ£€æŸ¥è®¾å¤‡
    check_device
    
    # è·å–è®¾å¤‡æ¶æ„
    DEVICE_ARCH=$(get_device_arch)
    echo "è®¾å¤‡æ¶æ„: ${DEVICE_ARCH}"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”æ¶æ„çš„ç¼–è¯‘æ–‡ä»¶
    if [ ! -f "android_executables/${DEVICE_ARCH}/mnist_inference" ]; then
        echo "é”™è¯¯: æ²¡æœ‰æ‰¾åˆ° ${DEVICE_ARCH} æ¶æ„çš„ç¼–è¯‘æ–‡ä»¶"
        echo "è¯·å…ˆè¿è¡Œç¼–è¯‘è„šæœ¬: ./build/build_executable.sh"
        exit 1
    fi
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    generate_test_data
    
    # éƒ¨ç½²åˆ°è®¾å¤‡
    deploy_to_device ${DEVICE_ARCH}
    
    # è¿è¡Œè®¾å¤‡æµ‹è¯•
    run_device_test
    
    # å¯¹æ¯”ç»“æœ
    compare_results
    
    echo "=== éƒ¨ç½²æµ‹è¯•å®Œæˆ ==="
}

# æ‰§è¡Œä¸»å‡½æ•°
main
```

## 10. å®Œæ•´è¿è¡Œè„šæœ¬

### 10.1 ä¸€é”®è¿è¡Œè„šæœ¬

```bash
#!/bin/bash
# run_complete_pipeline.sh

set -e

echo "=== MNISTå®Œæ•´éƒ¨ç½²æµç¨‹ ==="
echo "å¼€å§‹æ—¶é—´: $(date)"

# åˆ›å»ºé¡¹ç›®ç»“æ„
setup_project() {
    echo "1. è®¾ç½®é¡¹ç›®ç»“æ„..."
    mkdir -p {train,inference,build,models,data,results,android_libs,android_executables}
}

# è®­ç»ƒæ¨¡å‹
train_model() {
    echo "2. è®­ç»ƒMNISTæ¨¡å‹..."
    cd train
    python train_model.py
    cd ..
}

# é‡åŒ–æ¨¡å‹
quantize_model() {
    echo "3. é‡åŒ–æ¨¡å‹..."
    cd train
    python quantize_model.py
    cd ..
}

# å¯¼å‡ºONNX
export_onnx() {
    echo "4. å¯¼å‡ºONNXæ¨¡å‹..."
    cd train
    python export_onnx.py
    cd ..
}

# å¯è§†åŒ–æ¨¡å‹
visualize_model() {
    echo "5. å¯åŠ¨Netronå¯è§†åŒ– (åå°è¿è¡Œ)..."
    python -c "
import netron
import threading
import time

def start_netron():
    netron.start('./models/mnist_model.onnx', port=8080, browse=True)

thread = threading.Thread(target=start_netron, daemon=True)
thread.start()
print('Netronå·²å¯åŠ¨: http://localhost:8080')
time.sleep(2)
" &
}

# Pythonæ¨ç†
python_inference() {
    echo "6. Python ONNXæ¨ç†..."
    cd inference
    python python_inference.py
    cd ..
}

# ç¼–è¯‘C++ç‰ˆæœ¬
build_cpp() {
    echo "7. ç¼–è¯‘C++ç‰ˆæœ¬..."
    cd build
    
    # ä¸‹è½½ONNX Runtime for macOS
    if [ ! -d "onnxruntime-osx-x86_64-1.16.0" ]; then
        echo "ä¸‹è½½ONNX Runtime for macOS..."
        wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-osx-x86_64-1.16.0.tgz
        tar -xzf onnxruntime-osx-x86_64-1.16.0.tgz
        rm onnxruntime-osx-x86_64-1.16.0.tgz
    fi
    
    mkdir -p build_macos
    cd build_macos
    cmake -DCMAKE_BUILD_TYPE=Release ..
    make -j$(sysctl -n hw.ncpu)
    cd ../..
}

# C++æ¨ç†
cpp_inference() {
    echo "8. C++æ¨ç†æµ‹è¯•..."
    cd build/build_macos
    ./mnist_inference_cpp
    cd ../..
}

# ç¼–è¯‘Androidç‰ˆæœ¬
build_android() {
    echo "9. ç¼–è¯‘Androidç‰ˆæœ¬..."
    cd build
    chmod +x build_android.sh build_executable.sh
    ./build_executable.sh
    cd ..
}

# éƒ¨ç½²åˆ°æ‰‹æœº
deploy_to_phone() {
    echo "10. éƒ¨ç½²åˆ°æ‰‹æœºå¹¶æµ‹è¯•..."
    chmod +x deploy_and_test.sh
    ./deploy_and_test.sh
}

# ä¸»å‡½æ•°
main() {
    # æ£€æŸ¥ä¾èµ–
    echo "æ£€æŸ¥ä¾èµ–..."
    python -c "import torch, torchvision, onnx, onnxruntime, netron; print('Pythonä¾èµ–æ£€æŸ¥é€šè¿‡')"
    
    if [ -z "$ANDROID_NDK_HOME" ]; then
        echo "è­¦å‘Š: ANDROID_NDK_HOMEæœªè®¾ç½®ï¼ŒAndroidç¼–è¯‘å°†è¢«è·³è¿‡"
        SKIP_ANDROID=true
    fi
    
    # æ‰§è¡Œæµç¨‹
    setup_project
    train_model
    quantize_model
    export_onnx
    visualize_model
    python_inference
    build_cpp
    cpp_inference
    
    if [ "$SKIP_ANDROID" != "true" ]; then
        build_android
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Androidè®¾å¤‡
        if adb devices | grep -q "device$"; then
            deploy_to_phone
        else
            echo "æ²¡æœ‰æ£€æµ‹åˆ°Androidè®¾å¤‡ï¼Œè·³è¿‡æ‰‹æœºéƒ¨ç½²"
        fi
    fi
    
    echo "=== å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ ==="
    echo "ç»“æŸæ—¶é—´: $(date)"
    
    # æ˜¾ç¤ºç»“æœæ€»ç»“
    echo ""
    echo "=== ç»“æœæ–‡ä»¶ ==="
    echo "æ¨¡å‹æ–‡ä»¶:"
    ls -la models/
    echo ""
    echo "æ¨ç†ç»“æœ:"
    ls -la results/
    echo ""
    echo "Androidåº“æ–‡ä»¶:"
    if [ -d "android_libs" ]; then
        find android_libs -name "*.so" -o -name "*.a"
    fi
    echo ""
    echo "Androidå¯æ‰§è¡Œæ–‡ä»¶:"
    if [ -d "android_executables" ]; then
        find android_executables -name "mnist_inference"
    fi
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

### 10.2 ä½¿ç”¨è¯´æ˜

```bash
# 1. å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir mnist_complete_project
cd mnist_complete_project

# 2. å¤åˆ¶æ‰€æœ‰è„šæœ¬æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•

# 3. è®¾ç½®Android NDK (å¦‚æœéœ€è¦Androidç¼–è¯‘)
export ANDROID_NDK_HOME=/path/to/android-ndk

# 4. è¿æ¥Androidè®¾å¤‡ (å¦‚æœéœ€è¦æ‰‹æœºæµ‹è¯•)
adb devices

# 5. è¿è¡Œå®Œæ•´æµç¨‹
chmod +x run_complete_pipeline.sh
./run_complete_pipeline.sh
```

è¿™ä¸ªå®Œæ•´çš„æµç¨‹åŒ…æ‹¬ï¼š

1. âœ… **PyTorchè®­ç»ƒ** - è®­ç»ƒMNISTåˆ†ç±»æ¨¡å‹
2. âœ… **æ¨¡å‹é‡åŒ–** - ä½¿ç”¨PyTorché‡åŒ–API
3. âœ… **ONNXå¯¼å‡º** - è½¬æ¢ä¸ºONNXæ ¼å¼
4. âœ… **Netronå¯è§†åŒ–** - æŸ¥çœ‹æ¨¡å‹ç»“æ„
5. âœ… **Pythonæ¨ç†** - ONNX Runtime Python API
6. âœ… **C++æ¨ç†** - ONNX Runtime C++ API
7. âœ… **NDKç¼–è¯‘** - ç¼–è¯‘ARMåº“æ–‡ä»¶å’Œå¯æ‰§è¡Œæ–‡ä»¶
8. âœ… **æ‰‹æœºéƒ¨ç½²** - adbéƒ¨ç½²å¹¶è¿è¡Œæµ‹è¯•
9. âœ… **ç»“æœå¯¹æ¯”** - å¯¹æ¯”macOSå’ŒAndroidæ¨ç†ç»“æœ

æ•´ä¸ªæµç¨‹å®ç°äº†ä»æ¨¡å‹è®­ç»ƒåˆ°ç§»åŠ¨ç«¯éƒ¨ç½²çš„å®Œæ•´è‡ªåŠ¨åŒ–ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æ€§èƒ½å’Œå‡†ç¡®ç‡å¯¹æ¯”åˆ†æã€‚
