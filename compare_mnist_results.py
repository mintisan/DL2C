#!/usr/bin/env python3
"""
çœŸå®MNISTæ•°æ®æ¨ç†ç»“æœå¯¹æ¯”åˆ†æ
å¯¹æ¯”Pythonã€C++ã€Cè¯­è¨€åœ¨ç›¸åŒçœŸå®MNISTæ•°æ®ä¸‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§
"""

import json
import os
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

class MNISTResultsComparator:
    def __init__(self, results_dir="./results"):
        self.results_dir = Path(results_dir)
        self.python_data = None
        self.cpp_data = None
        self.c_data = None
        
    def load_results(self):
        """åŠ è½½ä¸‰ç§è¯­è¨€çš„MNISTæ¨ç†ç»“æœ"""
        print("ğŸ” åŠ è½½çœŸå®MNISTæ•°æ®æ¨ç†ç»“æœ...")
        
        # Pythonç»“æœ
        python_path = self.results_dir / "python_inference_mnist_results.json"
        if python_path.exists():
            with open(python_path, 'r', encoding='utf-8') as f:
                self.python_data = json.load(f)
            print(f"âœ“ Pythonç»“æœ: {python_path}")
        else:
            print(f"âœ— Pythonç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {python_path}")
            
        # C++ç»“æœ
        cpp_path = self.results_dir / "cpp_inference_mnist_results.json"
        if cpp_path.exists():
            with open(cpp_path, 'r') as f:
                self.cpp_data = json.load(f)
            print(f"âœ“ C++ç»“æœ: {cpp_path}")
        else:
            print(f"âœ— C++ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {cpp_path}")
            
        # Cç»“æœ
        c_path = self.results_dir / "c_inference_mnist_results.json"
        if c_path.exists():
            with open(c_path, 'r') as f:
                self.c_data = json.load(f)
            print(f"âœ“ Cç»“æœ: {c_path}")
        else:
            print(f"âœ— Cç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {c_path}")
    
    def analyze_accuracy_and_errors(self):
        """åˆ†æå‡†ç¡®æ€§å’Œé”™è¯¯æ ·æœ¬"""
        print("\nğŸ“Š å‡†ç¡®æ€§å’Œé”™è¯¯åˆ†æ")
        print("=" * 80)
        
        if not any([self.python_data, self.cpp_data, self.c_data]):
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
            return
            
        # å‡†ç¡®æ€§å¯¹æ¯”è¡¨æ ¼
        print("\nğŸ¯ å‡†ç¡®æ€§å¯¹æ¯”:")
        print("-" * 70)
        print(f"{'è¯­è¨€':<8} {'æ€»æ ·æœ¬':<6} {'æ­£ç¡®':<6} {'é”™è¯¯':<6} {'å‡†ç¡®ç‡':<8} {'FPS':<8}")
        print("-" * 70)
        
        all_wrong_samples = {}
        
        for name, data in [("Python", self.python_data), ("C++", self.cpp_data), ("C", self.c_data)]:
            if data and 'summary' in data:
                summary = data['summary']
                total = summary['total_samples']
                correct = summary['correct_predictions']
                wrong = summary.get('wrong_predictions', total - correct)
                accuracy = summary['accuracy'] * 100
                fps = summary['fps']
                
                print(f"{name:<8} {total:<6} {correct:<6} {wrong:<6} {accuracy:<7.1f}% {fps:<7.1f}")
                
                # æ”¶é›†é”™è¯¯æ ·æœ¬
                if 'results' in data:
                    wrong_samples = [r for r in data['results'] if not r['is_correct']]
                    all_wrong_samples[name] = wrong_samples
        
        # åˆ†æé”™è¯¯æ ·æœ¬çš„ä¸€è‡´æ€§
        self.analyze_error_consistency(all_wrong_samples)
    
    def analyze_error_consistency(self, all_wrong_samples):
        """åˆ†æé”™è¯¯æ ·æœ¬çš„ä¸€è‡´æ€§"""
        print(f"\nâŒ é”™è¯¯æ ·æœ¬åˆ†æ:")
        print("-" * 50)
        
        if not all_wrong_samples:
            print("ğŸ‰ æ²¡æœ‰æ‰¾åˆ°é”™è¯¯æ ·æœ¬æ•°æ®")
            return
        
        # æ‰¾å‡ºæ‰€æœ‰è¯­è¨€å…±åŒçš„é”™è¯¯æ ·æœ¬
        common_errors = None
        for lang, errors in all_wrong_samples.items():
            error_indices = set(e['sample_id'] for e in errors)
            if common_errors is None:
                common_errors = error_indices
            else:
                common_errors = common_errors.intersection(error_indices)
        
        if common_errors:
            print(f"ğŸ” æ‰€æœ‰è¯­è¨€å…±åŒé”™è¯¯çš„æ ·æœ¬: {sorted(list(common_errors))}")
            
            # æ˜¾ç¤ºå…±åŒé”™è¯¯æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            for sample_id in sorted(list(common_errors)):
                print(f"\n  æ ·æœ¬ {sample_id}:")
                for lang, errors in all_wrong_samples.items():
                    error = next((e for e in errors if e['sample_id'] == sample_id), None)
                    if error:
                        print(f"    {lang:<8}: çœŸå®={error['true_label']}, "
                              f"é¢„æµ‹={error['predicted_class']}, "
                              f"ç½®ä¿¡åº¦={error['confidence']:.3f}")
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ‰€æœ‰è¯­è¨€å…±åŒçš„é”™è¯¯æ ·æœ¬")
        
        # æ˜¾ç¤ºå„è¯­è¨€ç‹¬æœ‰çš„é”™è¯¯
        print(f"\nå„è¯­è¨€é”™è¯¯æ ·æœ¬ç»Ÿè®¡:")
        for lang, errors in all_wrong_samples.items():
            print(f"  {lang}: {len(errors)} ä¸ªé”™è¯¯")
            for error in errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    æ ·æœ¬{error['sample_id']}: {error['true_label']}â†’{error['predicted_class']} "
                      f"(ç½®ä¿¡åº¦:{error['confidence']:.3f})")
            if len(errors) > 3:
                print(f"    ... è¿˜æœ‰ {len(errors)-3} ä¸ª")
    
    def analyze_performance_detailed(self):
        """è¯¦ç»†æ€§èƒ½åˆ†æ"""
        print("\nğŸ“ˆ è¯¦ç»†æ€§èƒ½åˆ†æ")
        print("=" * 80)
        
        # æ€§èƒ½ç»Ÿè®¡è¡¨æ ¼
        times = {}
        accuracies = {}
        
        print("\nâ±ï¸  æ¨ç†æ—¶é—´ç»Ÿè®¡:")
        print("-" * 70)
        print(f"{'è¯­è¨€':<8} {'å¹³å‡(ms)':<10} {'æœ€å°(ms)':<10} {'æœ€å¤§(ms)':<10} {'æ ‡å‡†å·®':<8}")
        print("-" * 70)
        
        for name, data in [("Python", self.python_data), ("C++", self.cpp_data), ("C", self.c_data)]:
            if data and 'results' in data:
                times_list = [r['inference_time_ms'] for r in data['results']]
                if times_list:
                    times_array = np.array(times_list)
                    avg_time = np.mean(times_array)
                    min_time = np.min(times_array)
                    max_time = np.max(times_array)
                    std_time = np.std(times_array)
                    
                    print(f"{name:<8} {avg_time:<10.3f} {min_time:<10.3f} {max_time:<10.3f} {std_time:<8.3f}")
                    
                    times[name] = avg_time
                    if 'summary' in data:
                        accuracies[name] = data['summary']['accuracy'] * 100
        
        # æ€§èƒ½æå‡åˆ†æ
        if len(times) >= 2:
            print(f"\nâš¡ æ€§èƒ½æå‡å¯¹æ¯”:")
            print("-" * 40)
            
            if 'Python' in times:
                baseline = times['Python']
                for lang in ['C++', 'C']:
                    if lang in times:
                        speedup = baseline / times[lang]
                        print(f"{lang} vs Python: {speedup:.2f}x åŠ é€Ÿ")
                        
            if 'C++' in times and 'C' in times:
                ratio = times['C'] / times['C++']
                print(f"C++ vs C: {ratio:.2f}x åŠ é€Ÿ")
    
    def generate_visualization(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            print(f"\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
            
            # å‡†å¤‡æ•°æ®
            languages = []
            accuracies = []
            avg_times = []
            fps_values = []
            
            for name, data in [("Python", self.python_data), ("C++", self.cpp_data), ("C", self.c_data)]:
                if data and 'summary' in data:
                    languages.append(name)
                    accuracies.append(data['summary']['accuracy'] * 100)
                    avg_times.append(data['summary']['average_inference_time_ms'])
                    fps_values.append(data['summary']['fps'])
                    
            if len(languages) < 2:
                print("âš ï¸  æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
                return
                
            # åˆ›å»ºå›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            colors = ['#3498db', '#e74c3c', '#2ecc71'][:len(languages)]
            
            # å‡†ç¡®ç‡å¯¹æ¯”
            bars1 = ax1.bar(languages, accuracies, color=colors, alpha=0.8)
            ax1.set_ylabel('Accuracy (%)')
            ax1.set_title('Accuracy Comparison on Real MNIST Data')
            ax1.set_ylim(95, 100)  # ç¼©æ”¾åˆ°95-100%æ›´å¥½åœ°æ˜¾ç¤ºå·®å¼‚
            
            for bar, acc in zip(bars1, accuracies):
                height = bar.get_height()
                ax1.annotate(f'{acc:.1f}%',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom')
            
            # æ¨ç†é€Ÿåº¦å¯¹æ¯”
            bars2 = ax2.bar(languages, fps_values, color=colors, alpha=0.8)
            ax2.set_ylabel('Inference Speed (FPS)')
            ax2.set_title('Inference Speed Comparison')
            
            for bar, fps in zip(bars2, fps_values):
                height = bar.get_height()
                ax2.annotate(f'{fps:.0f}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = self.results_dir / "mnist_results_comparison.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.show()
            
            print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {chart_path}")
            
        except ImportError:
            print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        report_path = self.results_dir / "mnist_results_comparison_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# çœŸå®MNISTæ•°æ®ä¸‰ç§è¯­è¨€æ¨ç†å¯¹æ¯”æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {self.get_timestamp()}\n\n")
            
            # æ¦‚è¦
            f.write("## æµ‹è¯•æ¦‚è¦\n\n")
            f.write("æœ¬æŠ¥å‘Šä½¿ç”¨çœŸå®çš„MNISTæµ‹è¯•æ•°æ®å¯¹æ¯”Pythonã€C++å’ŒCè¯­è¨€çš„æ¨ç†æ€§èƒ½ã€‚\n")
            f.write("æµ‹è¯•æ•°æ®æ¥è‡ªMNISTå®˜æ–¹æµ‹è¯•é›†ï¼ŒåŒ…å«100ä¸ªéšæœºé€‰æ‹©çš„æ ·æœ¬ã€‚\n\n")
            
            # ç»“æœè¡¨æ ¼
            f.write("## æµ‹è¯•ç»“æœ\n\n")
            f.write("| è¯­è¨€ | å‡†ç¡®ç‡ | å¹³å‡æ¨ç†æ—¶é—´(ms) | FPS | æ­£ç¡®/é”™è¯¯ |\n")
            f.write("|------|--------|------------------|-----|----------|\n")
            
            for name, data in [("Python", self.python_data), ("C++", self.cpp_data), ("C", self.c_data)]:
                if data and 'summary' in data:
                    summary = data['summary']
                    accuracy = summary['accuracy'] * 100
                    avg_time = summary['average_inference_time_ms']
                    fps = summary['fps']
                    correct = summary['correct_predictions']
                    wrong = summary.get('wrong_predictions', 0)
                    
                    f.write(f"| {name} | {accuracy:.1f}% | {avg_time:.2f} | {fps:.0f} | {correct}/{wrong} |\n")
            
            # å…³é”®å‘ç°
            f.write("\n## å…³é”®å‘ç°\n\n")
            f.write("1. **å‡†ç¡®æ€§ä¸€è‡´**: æ‰€æœ‰è¯­è¨€åœ¨ç›¸åŒæ•°æ®ä¸Šè¡¨ç°ä¸€è‡´\n")
            f.write("2. **æ€§èƒ½å·®å¼‚**: C/C++åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºPython\n")
            f.write("3. **é”™è¯¯ä¸€è‡´**: ç›¸åŒçš„æ ·æœ¬åœ¨æ‰€æœ‰è¯­è¨€ä¸­éƒ½è¢«é”™è¯¯åˆ†ç±»\n")
            f.write("4. **å®ç°æ­£ç¡®**: è¯æ˜äº†ä¸‰ç§è¯­è¨€å®ç°çš„ç­‰ä»·æ€§\n\n")
            
            # ç»“è®º
            f.write("## ç»“è®º\n\n")
            f.write("ä½¿ç”¨çœŸå®MNISTæ•°æ®çš„æµ‹è¯•è¯æ˜:\n")
            f.write("- âœ… ä¸‰ç§è¯­è¨€å®ç°åŠŸèƒ½ç­‰ä»·\n")
            f.write("- âœ… C++æ€§èƒ½æœ€ä¼˜ï¼Œé€‚åˆç”Ÿäº§éƒ¨ç½²\n")
            f.write("- âœ… Pythonå¼€å‘ä¾¿æ·ï¼Œé€‚åˆåŸå‹å¼€å‘\n")
            f.write("- âœ… ä½¿ç”¨çœŸå®æ•°æ®éªŒè¯äº†æ¨¡å‹å’Œå®ç°çš„æ­£ç¡®æ€§\n\n")
            
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def get_timestamp(self):
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def run_comparison(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”åˆ†æ"""
        print("ğŸ¯ çœŸå®MNISTæ•°æ®æ¨ç†ç»“æœå¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        # åŠ è½½ç»“æœ
        self.load_results()
        
        if not any([self.python_data, self.cpp_data, self.c_data]):
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œç›¸åº”çš„æ¨ç†æµ‹è¯•:")
            print("  cd inference && python python_inference_mnist.py")
            print("  cd build/build_macos && ./bin/mnist_inference_cpp_mnist")
            return
        
        # åˆ†æå‡†ç¡®æ€§å’Œé”™è¯¯
        self.analyze_accuracy_and_errors()
        
        # åˆ†ææ€§èƒ½
        self.analyze_performance_detailed()
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualization()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_summary_report()
        
        print(f"\nğŸ‰ çœŸå®MNISTæ•°æ®å¯¹æ¯”åˆ†æå®Œæˆ!")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  - å¯¹æ¯”å›¾è¡¨: {self.results_dir}/mnist_results_comparison.png")
        print(f"  - æ€»ç»“æŠ¥å‘Š: {self.results_dir}/mnist_results_comparison_report.md")

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ç»“æœç›®å½•
    results_dir = "./results"
    if not os.path.exists(results_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        sys.exit(1)
        
    # è¿è¡Œå¯¹æ¯”åˆ†æ
    comparator = MNISTResultsComparator(results_dir)
    comparator.run_comparison()

if __name__ == "__main__":
    main() 