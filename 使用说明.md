# MNIST模型部署教学项目

这是一个完整的MNIST手写数字识别模型部署教学项目，展示了从PyTorch训练到跨平台部署的完整流程。

## 项目结构

```
DL2C/
├── train/                          # 训练相关脚本
│   ├── train_model.py              # 模型训练
│   ├── quantize_model.py           # 模型量化
│   └── export_onnx.py              # ONNX导出
├── inference/                      # 推理相关代码
│   ├── python_inference.py        # Python推理（基础版本）
│   ├── cpp_inference.cpp          # C++推理（基础版本）
│   ├── c_inference.c              # C语言推理（基础版本）
│   ├── python_inference_common.py # Python推理（共同数据版本）
│   ├── cpp_inference_common.cpp   # C++推理（共同数据版本）
│   ├── c_inference_common.c       # C语言推理（共同数据版本）
│   ├── python_inference_mnist.py  # Python推理（真实MNIST版本）
│   ├── cpp_inference_mnist.cpp    # C++推理（真实MNIST版本）
│   └── c_inference_mnist.c        # C语言推理（真实MNIST版本）
├── build/                          # 编译配置
│   └── CMakeLists.txt              # CMake配置
├── models/                         # 模型文件目录
├── data/                           # 数据目录
├── results/                        # 结果文件目录
├── test_data_mnist/                # 真实MNIST测试数据目录
├── run_tutorial.py                 # 教学脚本
├── compare_languages.py            # 基础版本性能对比脚本
├── compare_common_data.py          # 共同数据性能对比脚本
├── compare_mnist_results.py        # 真实MNIST数据对比脚本
├── generate_common_test_data.py    # 生成共同测试数据脚本
├── mnist_data_loader.py            # 真实MNIST数据加载脚本
└── 使用说明.md                     # 本文件
```

## 快速开始

### 方法一：交互式教学（推荐）

直接运行教学脚本，它会一步一步引导您完成整个流程：

```bash
python run_tutorial.py
```

这个脚本会：
- 检查依赖环境
- 逐步执行每个步骤
- 提供详细的说明和错误处理
- 支持失败重试

### 方法二：手动执行

如果您想手动执行每个步骤，请按以下顺序：

#### 1. 安装依赖

```bash
pip install torch torchvision onnx onnxruntime numpy matplotlib Pillow
```

#### 2. 训练模型

```bash
cd train
python train_model.py
```

#### 3. 量化模型

```bash
python quantize_model.py
```

#### 4. 导出ONNX

```bash
python export_onnx.py
```

#### 5. Python推理测试

我们提供了三种不同的推理版本，适用于不同的测试场景：

**版本一：基础推理（原始版本）**
```bash
cd ../inference
python python_inference.py
```
- 使用随机生成或PIL图像数据
- 适合快速验证模型是否正常工作
- 主要用于功能测试

**版本二：共同数据推理**
```bash
# 首先生成共同测试数据（如果还没有）
python generate_common_test_data.py

# 运行共同数据推理
python python_inference_common.py
```
- 使用预先生成的固定测试数据集
- 确保多种语言使用完全相同的输入
- 适合语言间一致性验证

**版本三：真实MNIST推理（推荐）**
```bash
# 首先生成真实MNIST测试数据
python mnist_data_loader.py

# 运行真实MNIST推理
python python_inference_mnist.py
```
- 使用真实的MNIST测试集数据
- 提供最真实的性能评估
- 符合学术和工业界标准

#### 6. 安装ONNX Runtime C++库

使用Homebrew安装（推荐）：
```bash
brew install onnxruntime
```

或从官网下载预编译版本：
https://github.com/microsoft/onnxruntime/releases

#### 7. 编译C++版本

```bash
cd ../build
mkdir build_macos && cd build_macos
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j4
```

#### 8. 运行C++推理

同样提供三种不同版本：

**版本一：基础推理（原始版本）**
```bash
./bin/mnist_inference_cpp
```

**版本二：共同数据推理**
```bash
./bin/mnist_inference_cpp_common
```

**版本三：真实MNIST推理（推荐）**
```bash
./bin/mnist_inference_cpp_mnist
```

#### 9. 运行C语言推理

对应的C语言版本：

**版本一：基础推理（原始版本）**
```bash
./bin/mnist_inference_c
```

**版本二：共同数据推理**
```bash
./bin/mnist_inference_c_common
```

**版本三：真实MNIST推理（推荐）**
```bash
./bin/mnist_inference_c_mnist
```

#### 10. 三种语言性能对比

根据使用的数据版本，我们提供了不同的对比脚本：

**基础性能对比**
```bash
cd ../..
python compare_languages.py
```
- 对比基础版本的推理结果
- 主要关注FPS性能

**共同数据对比**
```bash
python compare_common_data.py
```
- 对比使用相同输入数据的结果
- 验证实现一致性和性能差异

**真实MNIST数据对比（推荐）**
```bash
python compare_mnist_results.py
```
- 基于真实MNIST测试数据的全面对比
- 包括准确率、性能、错误样本分析
- 生成详细的可视化报告

## 学习要点

### 1. PyTorch模型训练
- 简单的CNN架构设计
- 数据预处理和增强
- 训练循环和验证

### 2. 模型量化
- 动态量化技术
- 精度vs性能权衡
- 量化前后对比

### 3. ONNX导出
- 跨平台模型格式
- 模型验证和测试
- 动态维度支持

### 4. 推理引擎与测试版本
- **基础版本**: 快速功能验证，使用随机或简单数据
- **共同数据版本**: 多语言一致性测试，使用相同输入数据
- **真实MNIST版本**: 标准基准测试，使用真实MNIST数据集
- Python/C++/C三种语言API的使用
- 全面的性能和准确性对比分析

### 5. 跨平台部署
- macOS本地开发
- C++编译配置
- 性能优化选项

## 常见问题

### Q: 训练很慢怎么办？
A: 可以减少epochs数量（在`train_model.py`中修改），或使用更小的数据集进行快速验证。

### Q: ONNX Runtime找不到怎么办？
A: 
1. 首先尝试`brew install onnxruntime`
2. 如果失败，从GitHub下载预编译版本
3. 确保路径配置正确

### Q: C++/C编译失败怎么办？
A: 
1. 检查是否安装了ONNX Runtime
2. 确保CMake和编译器版本兼容
3. 查看详细错误信息
4. 对于C语言版本，确保链接了数学库(-lm)

### Q: 推理结果不一致怎么办？
A: 
1. 检查预处理步骤是否一致
2. 确认数据类型和维度匹配
3. 验证ONNX模型导出是否正确

### Q: 应该选择哪种语言进行部署？
A: 
1. **开发阶段**: Python (快速原型和验证)
2. **生产部署**: C/C++ (高性能和低资源占用)
3. **跨平台兼容**: C语言 (最广泛的兼容性)
4. **移动端**: C语言 (最小的运行时依赖)

### Q: 应该选择哪个推理版本？
A: 
1. **基础版本**: 
   - 适合快速功能验证
   - 检查模型是否正常加载和运行
   - 开发阶段的初步测试
2. **共同数据版本**: 
   - 验证多语言实现的一致性
   - 确保所有语言产生相同结果
   - 调试和验证阶段使用
3. **真实MNIST版本（推荐）**: 
   - 提供最准确的性能评估
   - 使用标准基准数据集
   - 适合正式的性能报告和比较
   - 符合学术和工业界标准

### Q: 三种版本的性能结果为什么不同？
A: 
1. **数据差异**: 不同版本使用不同的测试数据
2. **数据量差异**: 真实MNIST版本测试更多样本
3. **预处理差异**: 真实数据需要标准化处理
4. **评估维度**: 真实版本包含准确率评估

## 扩展学习

完成基础教程后，您可以尝试：

1. **数据集扩展**: 
   - 修改`mnist_data_loader.py`使用更多样本（1000或10000个）
   - 测试完整MNIST测试集的性能表现
   - 分析不同数据量对性能的影响

2. **自定义数据测试**: 
   - 使用自己的手写数字图片
   - 修改预处理流程适配新数据
   - 验证模型的泛化能力

3. **性能深度分析**: 
   - 分析三种语言的内存使用情况
   - 测试多线程推理性能
   - 比较不同ONNX Runtime优化选项

4. **Android/移动端部署**: 
   - 使用NDK编译ARM版本
   - 集成到Android应用
   - 测试移动端性能表现

5. **模型优化实验**: 
   - 尝试不同的量化方法
   - 使用ONNX优化工具
   - 比较模型大小和精度权衡

6. **实际应用开发**: 
   - 集成到手机或Web应用
   - 开发实时手写数字识别系统
   - 添加用户界面和交互功能

## 技术栈

- **训练框架**: PyTorch
- **推理引擎**: ONNX Runtime
- **编程语言**: Python, C++
- **构建系统**: CMake
- **平台支持**: macOS, Linux, Android

## 参考资源

- [PyTorch官方文档](https://pytorch.org/docs/)
- [ONNX Runtime文档](https://onnxruntime.ai/docs/)
- [ONNX格式规范](https://onnx.ai/)
- [CMake教程](https://cmake.org/cmake/help/latest/guide/tutorial/)

祝您学习愉快！🎓 