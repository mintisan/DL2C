import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torchvision
import os
import numpy as np
from train_model import MNISTNet

def simulate_quantization(model, bits=8):
    """
    æ¨¡æ‹Ÿé‡åŒ–ï¼šé€šè¿‡é™åˆ¶æƒé‡ç²¾åº¦æ¥æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
    è¿™æ˜¯ä¸€ä¸ªæ•™å­¦ç”¨çš„ç®€åŒ–é‡åŒ–å®ç°
    """
    print(f"æ¨¡æ‹Ÿ {bits}-bit é‡åŒ–...")
    
    # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
    quantized_model = MNISTNet()
    quantized_model.load_state_dict(model.state_dict())
    quantized_model.eval()
    
    # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œ"é‡åŒ–"
    with torch.no_grad():
        for name, param in quantized_model.named_parameters():
            if 'weight' in name:
                # è®¡ç®—é‡åŒ–èŒƒå›´
                param_min = param.min()
                param_max = param.max()
                
                # é‡åŒ–åˆ°æŒ‡å®šä½æ•°
                levels = 2 ** bits
                scale = (param_max - param_min) / (levels - 1)
                
                # é‡åŒ–å’Œåé‡åŒ–
                quantized_param = torch.round((param - param_min) / scale) * scale + param_min
                param.copy_(quantized_param)
                
                print(f"é‡åŒ–å‚æ•° {name}: èŒƒå›´ [{param_min:.4f}, {param_max:.4f}], ç¼©æ”¾ {scale:.6f}")
    
    return quantized_model

def get_model_size(model):
    """è®¡ç®—æ¨¡å‹å¤§å°"""
    param_size = 0
    buffer_size = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

def test_model_accuracy(model, data_loader):
    """æµ‹è¯•æ¨¡å‹ç²¾åº¦"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in data_loader:
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    return 100 * correct / total

def quantize_model():
    """æ‰§è¡Œæ¨¡æ‹Ÿé‡åŒ–"""
    print("å¼€å§‹æ¨¡å‹é‡åŒ–ï¼ˆå…¼å®¹æ€§ç‰ˆæœ¬ï¼‰...")
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    print("åŠ è½½åŸå§‹æ¨¡å‹...")
    model = MNISTNet()
    model.load_state_dict(torch.load('../models/mnist_model.pth', map_location='cpu', weights_only=True))
    model.eval()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = torchvision.datasets.MNIST(
        root='../data', train=False, download=False, transform=transform
    )
    
    # ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•
    test_subset = torch.utils.data.Subset(test_dataset, range(1000))
    test_loader = DataLoader(test_subset, batch_size=100, shuffle=False)
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹
    print("æµ‹è¯•åŸå§‹æ¨¡å‹ç²¾åº¦...")
    original_accuracy = test_model_accuracy(model, test_loader)
    
    # æ‰§è¡Œæ¨¡æ‹Ÿé‡åŒ–
    print("æ‰§è¡Œæ¨¡æ‹Ÿé‡åŒ–...")
    quantized_model = simulate_quantization(model, bits=8)
    
    # æµ‹è¯•é‡åŒ–æ¨¡å‹
    print("æµ‹è¯•é‡åŒ–æ¨¡å‹ç²¾åº¦...")
    quantized_accuracy = test_model_accuracy(quantized_model, test_loader)
    
    # è®¡ç®—æ¨¡å‹å¤§å°
    original_size = get_model_size(model)
    quantized_size = get_model_size(quantized_model)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\n=== æ¨¡æ‹Ÿé‡åŒ–ç»“æœå¯¹æ¯” ===")
    print(f"é‡åŒ–æ–¹æ³•: 8-bit æ¨¡æ‹Ÿé‡åŒ– (å…¼å®¹æ€§ç‰ˆæœ¬)")
    print(f"åŸå§‹æ¨¡å‹ç²¾åº¦: {original_accuracy:.2f}%")
    print(f"é‡åŒ–æ¨¡å‹ç²¾åº¦: {quantized_accuracy:.2f}%")
    print(f"ç²¾åº¦æŸå¤±: {original_accuracy - quantized_accuracy:.2f}%")
    print(f"åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
    print(f"é‡åŒ–æ¨¡å‹å¤§å°: {quantized_size:.2f} MB")
    print(f"æ¨¡å‹å‹ç¼©æ¯”: {original_size/quantized_size:.2f}x")
    
    print(f"\nğŸ’¡ æ³¨æ„: è¿™æ˜¯æ•™å­¦ç”¨çš„æ¨¡æ‹Ÿé‡åŒ–ï¼Œå®é™…éƒ¨ç½²å»ºè®®ä½¿ç”¨æ”¯æŒçš„å¹³å°è¿›è¡ŒçœŸå®é‡åŒ–")
    print(f"ğŸ“š åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨ ONNX Runtime çš„é‡åŒ–åŠŸèƒ½")
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    print("ä¿å­˜æ¨¡æ‹Ÿé‡åŒ–æ¨¡å‹...")
    os.makedirs('../models', exist_ok=True)
    torch.save(quantized_model.state_dict(), '../models/mnist_quantized.pth')
    torch.save(quantized_model, '../models/mnist_quantized_full.pth')
    print("é‡åŒ–æ¨¡å‹å·²ä¿å­˜åˆ° ../models/mnist_quantized.pth")
    
    return model, quantized_model

if __name__ == "__main__":
    try:
        original, quantized = quantize_model()
        print("âœ… æ¨¡æ‹Ÿé‡åŒ–å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ é‡åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨: ../models/mnist_model.pth")
        print("2. ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨: ../data/MNIST/")
        import sys
        sys.exit(1) 