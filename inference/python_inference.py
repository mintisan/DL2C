import onnxruntime as ort
import numpy as np
import torchvision
import torchvision.transforms as transforms
import torch
from PIL import Image
import time
import json
import os

class PythonONNXInference:
    """Python ONNXæ¨ç†ç±»"""
    
    def __init__(self, model_path):
        """åˆå§‹åŒ–ONNXæ¨ç†å¼•æ“"""
        print(f"åŠ è½½ONNXæ¨¡å‹: {model_path}")
        
        # åˆ›å»ºONNX Runtimeä¼šè¯ï¼ˆæ¨ç†ä½¿ç”¨CPUä»¥ç¡®ä¿å…¼å®¹æ€§ï¼‰
        providers = ['CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
        print("ğŸ–¥ï¸  æ¨ç†ä½¿ç”¨CPUæ‰§è¡Œï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§")
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        self.input_shape = self.session.get_inputs()[0].shape
        self.output_shape = self.session.get_outputs()[0].shape
        
        print(f"è¾“å…¥åç§°: {self.input_name}")
        print(f"è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        print(f"è¾“å‡ºåç§°: {self.output_name}")
        print(f"è¾“å‡ºå½¢çŠ¶: {self.output_shape}")
        
    def preprocess(self, image):
        """é¢„å¤„ç†å›¾åƒæ•°æ®"""
        if isinstance(image, Image.Image):
            # PILå›¾åƒå¤„ç†
            transform = transforms.Compose([
                transforms.Grayscale(),
                transforms.Resize((28, 28)),
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
            tensor = transform(image)
            return tensor.unsqueeze(0).numpy()
        elif isinstance(image, np.ndarray):
            # NumPyæ•°ç»„å¤„ç†
            if image.ndim == 2:  # å•é€šé“å›¾åƒ
                image = image.reshape(1, 1, 28, 28)
            image = image.astype(np.float32) / 255.0
            # æ ‡å‡†åŒ–
            image = (image - 0.1307) / 0.3081
            return image
        elif isinstance(image, torch.Tensor):
            # PyTorchå¼ é‡å¤„ç†
            if image.dim() == 3:  # [C, H, W] -> [1, C, H, W]
                image = image.unsqueeze(0)
            elif image.dim() == 2:  # [H, W] -> [1, 1, H, W]
                image = image.unsqueeze(0).unsqueeze(0)
            return image.numpy().astype(np.float32)
        else:
            return image
    
    def postprocess(self, output):
        """åå¤„ç†è¾“å‡ºç»“æœ"""
        # è·å–ONNXè¾“å‡º (å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, num_classes])
        logits = output[0]
        
        # å¦‚æœæ˜¯æ‰¹æ¬¡è¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ªæ ·æœ¬
        if logits.ndim == 2:
            logits = logits[0]  # ä» [1, 10] å˜ä¸º [10]
        
        # åº”ç”¨softmaxè·å¾—æ¦‚ç‡åˆ†å¸ƒ
        exp_logits = np.exp(logits - np.max(logits))  # æ•°å€¼ç¨³å®šæ€§
        probabilities = exp_logits / np.sum(exp_logits)
        
        # è·å–é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦
        predicted_class = np.argmax(probabilities)
        confidence = probabilities[predicted_class]
        
        return {
            'predicted_class': int(predicted_class),
            'confidence': float(confidence),
            'probabilities': probabilities.tolist(),
            'raw_logits': logits.tolist()
        }
    
    def inference(self, input_data):
        """æ‰§è¡Œæ¨ç†"""
        start_time = time.time()
        
        # é¢„å¤„ç†
        processed_input = self.preprocess(input_data)
        
        # è¿è¡Œæ¨ç†
        ort_inputs = {self.input_name: processed_input}
        ort_outputs = self.session.run([self.output_name], ort_inputs)
        
        # åå¤„ç†
        result = self.postprocess(ort_outputs)
        
        # æ·»åŠ æ¨ç†æ—¶é—´
        inference_time = time.time() - start_time
        result['inference_time_ms'] = inference_time * 1000
        
        return result
    
    def batch_inference(self, input_batch):
        """æ‰¹é‡æ¨ç†"""
        results = []
        for i, input_data in enumerate(input_batch):
            result = self.inference(input_data)
            result['batch_index'] = i
            results.append(result)
        return results

def test_python_inference():
    """æµ‹è¯•Python ONNXæ¨ç†"""
    print("=== Python ONNXæ¨ç†æµ‹è¯• ===")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = '../models/mnist_model.onnx'
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒå’ŒONNXå¯¼å‡ºè„šæœ¬")
        return None
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    inference_engine = PythonONNXInference(model_path)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\nåŠ è½½MNISTæµ‹è¯•æ•°æ®...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = torchvision.datasets.MNIST(
        root='../data', train=False, download=False, transform=transform
    )
    
    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    print("\n=== å•æ ·æœ¬æ¨ç†æµ‹è¯• ===")
    test_samples = 10
    results = []
    
    for i in range(test_samples):
        image, true_label = test_dataset[i]
        
        # æ‰§è¡Œæ¨ç†
        result = inference_engine.inference(image)
        result['true_label'] = int(true_label)
        result['sample_id'] = i
        
        results.append(result)
        
        # æ˜¾ç¤ºç»“æœ
        status = "âœ“" if result['predicted_class'] == true_label else "âœ—"
        print(f"æ ·æœ¬ {i:2d}: çœŸå®={true_label}, é¢„æµ‹={result['predicted_class']}, "
              f"ç½®ä¿¡åº¦={result['confidence']:.4f}, æ—¶é—´={result['inference_time_ms']:.2f}ms {status}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n=== æ€§èƒ½ç»Ÿè®¡ ===")
    correct = sum(1 for r in results if r['predicted_class'] == r['true_label'])
    accuracy = correct / len(results)
    avg_time = np.mean([r['inference_time_ms'] for r in results])
    std_time = np.std([r['inference_time_ms'] for r in results])
    
    print(f"å‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{len(results)})")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"æ¨ç†é€Ÿåº¦: {1000/avg_time:.1f} FPS")
    
    # æµ‹è¯•æ‰¹é‡æ¨ç†
    print("\n=== æ‰¹é‡æ¨ç†æµ‹è¯• ===")
    batch_size = 32
    batch_data = [test_dataset[i][0] for i in range(batch_size)]
    batch_labels = [test_dataset[i][1] for i in range(batch_size)]
    
    start_time = time.time()
    batch_results = inference_engine.batch_inference(batch_data)
    batch_time = time.time() - start_time
    
    batch_correct = sum(1 for i, r in enumerate(batch_results) 
                       if r['predicted_class'] == batch_labels[i])
    batch_accuracy = batch_correct / len(batch_results)
    
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"æ‰¹é‡å‡†ç¡®ç‡: {batch_accuracy:.2%}")
    print(f"æ‰¹é‡æ€»æ—¶é—´: {batch_time*1000:.2f} ms")
    print(f"å¹³å‡å•æ ·æœ¬æ—¶é—´: {batch_time*1000/batch_size:.2f} ms")
    print(f"æ‰¹é‡æ¨ç†é€Ÿåº¦: {batch_size/batch_time:.1f} FPS")
    
    # ä¿å­˜ç»“æœ
    print("\nä¿å­˜æ¨ç†ç»“æœ...")
    os.makedirs('../results', exist_ok=True)
    
    summary_result = {
        'model_path': model_path,
        'test_samples': test_samples,
        'results': results,
        'summary': {
            'accuracy': accuracy,
            'average_inference_time_ms': avg_time,
            'std_inference_time_ms': std_time,
            'fps': 1000/avg_time,
            'total_samples': len(results),
            'correct_predictions': correct
        },
        'batch_test': {
            'batch_size': batch_size,
            'batch_accuracy': batch_accuracy,
            'batch_total_time_ms': batch_time * 1000,
            'batch_avg_time_per_sample_ms': batch_time * 1000 / batch_size,
            'batch_fps': batch_size / batch_time
        }
    }
    
    with open('../results/python_inference_results.json', 'w', encoding='utf-8') as f:
        json.dump(summary_result, f, indent=2, ensure_ascii=False)
    
    print("ç»“æœå·²ä¿å­˜åˆ°: ../results/python_inference_results.json")
    
    return summary_result

def test_different_inputs():
    """æµ‹è¯•ä¸åŒç±»å‹çš„è¾“å…¥"""
    print("\n=== æµ‹è¯•ä¸åŒè¾“å…¥ç±»å‹ ===")
    
    model_path = '../models/mnist_model.onnx'
    inference_engine = PythonONNXInference(model_path)
    
    # 1. æµ‹è¯•PILå›¾åƒ
    print("1. æµ‹è¯•PILå›¾åƒè¾“å…¥...")
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    test_image_pil = Image.new('L', (28, 28), 128)  # ç°è‰²å›¾åƒ
    result_pil = inference_engine.inference(test_image_pil)
    print(f"PILå›¾åƒæ¨ç†ç»“æœ: {result_pil['predicted_class']}, ç½®ä¿¡åº¦: {result_pil['confidence']:.4f}")
    
    # 2. æµ‹è¯•NumPyæ•°ç»„
    print("2. æµ‹è¯•NumPyæ•°ç»„è¾“å…¥...")
    test_image_np = np.random.rand(28, 28) * 255
    result_np = inference_engine.inference(test_image_np)
    print(f"NumPyæ•°ç»„æ¨ç†ç»“æœ: {result_np['predicted_class']}, ç½®ä¿¡åº¦: {result_np['confidence']:.4f}")
    
    # 3. æµ‹è¯•PyTorchå¼ é‡
    print("3. æµ‹è¯•PyTorchå¼ é‡è¾“å…¥...")
    test_image_torch = torch.randn(1, 28, 28)
    result_torch = inference_engine.inference(test_image_torch)
    print(f"PyTorchå¼ é‡æ¨ç†ç»“æœ: {result_torch['predicted_class']}, ç½®ä¿¡åº¦: {result_torch['confidence']:.4f}")

if __name__ == "__main__":
    # è¿è¡Œä¸»è¦æµ‹è¯•
    results = test_python_inference()
    
    if results:
        # æµ‹è¯•ä¸åŒè¾“å…¥ç±»å‹
        test_different_inputs()
        
        print("\n=== Pythonæ¨ç†æµ‹è¯•å®Œæˆ ===")
        print("æ¥ä¸‹æ¥å¯ä»¥è¿è¡ŒC++æ¨ç†æµ‹è¯•è¿›è¡Œå¯¹æ¯”")
    else:
        print("Pythonæ¨ç†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶") 