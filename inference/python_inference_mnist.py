#!/usr/bin/env python3
"""
Python ONNXæ¨ç† - ä½¿ç”¨çœŸå®MNISTæµ‹è¯•æ•°æ®
ä½¿ç”¨ä»åŸå§‹MNISTæµ‹è¯•é›†ä¸­é€‰æ‹©çš„çœŸå®æ ·æœ¬è¿›è¡Œæ¨ç†
"""

import onnxruntime as ort
import numpy as np
import json
import time
import os
from pathlib import Path

class PythonONNXInferenceMNIST:
    """Python ONNXæ¨ç†ç±» - ä½¿ç”¨çœŸå®MNISTæ•°æ®"""
    
    def __init__(self, model_path):
        """åˆå§‹åŒ–ONNXæ¨ç†å¼•æ“"""
        print(f"åŠ è½½ONNXæ¨¡å‹: {model_path}")
        
        # åˆ›å»ºONNX Runtimeä¼šè¯
        providers = ['CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
        print(f"âœ… Python ONNX Runtimeåˆå§‹åŒ–æˆåŠŸ")
        print(f"è¾“å…¥åç§°: {self.input_name}")
        print(f"è¾“å‡ºåç§°: {self.output_name}")
        
    def preprocess(self, image_data):
        """é¢„å¤„ç†å›¾åƒæ•°æ®"""
        # è¾“å…¥æ•°æ®å·²ç»æ˜¯[28, 28]çš„float32æ•°ç»„ï¼ŒèŒƒå›´[0,1]
        # éœ€è¦æ ‡å‡†åŒ–å¹¶è°ƒæ•´å½¢çŠ¶
        
        # æ ‡å‡†åŒ– (MNISTæ ‡å‡†åŒ–å‚æ•°)
        mean = 0.1307
        std = 0.3081
        normalized = (image_data - mean) / std
        
        # è°ƒæ•´å½¢çŠ¶ä¸º [1, 1, 28, 28]
        input_data = normalized.reshape(1, 1, 28, 28).astype(np.float32)
        
        return input_data
    
    def postprocess(self, output):
        """åå¤„ç†è¾“å‡ºç»“æœ"""
        # è·å–ONNXè¾“å‡º
        logits = output[0][0]  # ä» [1, 10] å˜ä¸º [10]
        
        # åº”ç”¨softmaxè·å¾—æ¦‚ç‡åˆ†å¸ƒ
        exp_logits = np.exp(logits - np.max(logits))
        probabilities = exp_logits / np.sum(exp_logits)
        
        # è·å–é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦
        predicted_class = np.argmax(probabilities)
        confidence = probabilities[predicted_class]
        
        return {
            'predicted_class': int(predicted_class),
            'confidence': float(confidence),
            'probabilities': probabilities.tolist(),
            'raw_logits': logits.tolist()
        }
    
    def inference(self, image_data):
        """æ‰§è¡Œæ¨ç†"""
        start_time = time.time()
        
        # é¢„å¤„ç†
        processed_input = self.preprocess(image_data)
        
        # è¿è¡Œæ¨ç†
        ort_inputs = {self.input_name: processed_input}
        ort_outputs = self.session.run([self.output_name], ort_inputs)
        
        end_time = time.time()
        inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # åå¤„ç†
        result = self.postprocess(ort_outputs)
        result['inference_time_ms'] = inference_time
        
        return result

def load_mnist_test_data():
    """åŠ è½½MNISTæµ‹è¯•æ•°æ®"""
    test_data_dir = Path("../test_data_mnist")
    npz_file = test_data_dir / "mnist_test_subset.npz"
    
    if not npz_file.exists():
        print("âŒ æ‰¾ä¸åˆ°MNISTæµ‹è¯•æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ mnist_data_loader.py")
        return None, None, None
    
    # åŠ è½½NPZæ•°æ®
    data = np.load(npz_file)
    images = data['images']  # shape: (num_samples, 28, 28)
    labels = data['labels']  # shape: (num_samples,)
    indices = data['indices']  # åŸå§‹MNISTç´¢å¼•
    
    print(f"ğŸ” åŠ è½½ {len(images)} ä¸ªMNISTæµ‹è¯•æ ·æœ¬")
    print(f"æ•°æ®å½¢çŠ¶: {images.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
    
    return images, labels, indices

def test_python_inference_mnist():
    """ä½¿ç”¨çœŸå®MNISTæ•°æ®è¿›è¡ŒPythonæ¨ç†æµ‹è¯•"""
    print("=== Python ONNXæ¨ç†æµ‹è¯• (çœŸå®MNISTæ•°æ®) ===")
    
    # åŠ è½½æ¨¡å‹
    model_path = '../models/mnist_model.onnx'
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    inference_engine = PythonONNXInferenceMNIST(model_path)
    
    # åŠ è½½MNISTæµ‹è¯•æ•°æ®
    images, labels, indices = load_mnist_test_data()
    if images is None:
        return None
    
    print(f"\nå¼€å§‹æ¨ç† {len(images)} ä¸ªæ ·æœ¬...")
    
    # æ‰§è¡Œæ¨ç†
    results = []
    correct_predictions = 0
    total_time = 0
    
    for i, (image_data, true_label, original_idx) in enumerate(zip(images, labels, indices)):
        # æ‰§è¡Œæ¨ç†
        result = inference_engine.inference(image_data)
        
        # æ£€æŸ¥å‡†ç¡®æ€§
        is_correct = result['predicted_class'] == true_label
        if is_correct:
            correct_predictions += 1
        
        # è®°å½•ç»“æœ
        sample_result = {
            'sample_id': i,
            'original_mnist_index': int(original_idx),
            'true_label': int(true_label),
            'predicted_class': result['predicted_class'],
            'confidence': result['confidence'],
            'inference_time_ms': result['inference_time_ms'],
            'is_correct': bool(is_correct),
            'probabilities': result['probabilities']
        }
        
        results.append(sample_result)
        total_time += result['inference_time_ms']
        
        # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªæ ·æœ¬ï¼‰
        if (i + 1) % 10 == 0:
            print(f"å®Œæˆ {i+1:3d}/{len(images)} æ ·æœ¬ï¼Œå½“å‰å‡†ç¡®ç‡: {correct_predictions/(i+1)*100:.1f}%")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    accuracy = correct_predictions / len(results)
    avg_time = total_time / len(results)
    std_time = np.std([r['inference_time_ms'] for r in results])
    
    print(f"\n=== æ¨ç†ç»“æœç»Ÿè®¡ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct_predictions}")
    print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
    print(f"æ—¶é—´æ ‡å‡†å·®: {std_time:.2f} ms")
    print(f"æ¨ç†é€Ÿåº¦: {1000/avg_time:.1f} FPS")
    
    # æ˜¾ç¤ºé”™è¯¯æ ·æœ¬
    wrong_samples = [r for r in results if not r['is_correct']]
    if wrong_samples:
        print(f"\nâŒ é”™è¯¯é¢„æµ‹æ ·æœ¬ ({len(wrong_samples)} ä¸ª):")
        for sample in wrong_samples[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  æ ·æœ¬ {sample['sample_id']:3d}: çœŸå®={sample['true_label']}, "
                  f"é¢„æµ‹={sample['predicted_class']}, ç½®ä¿¡åº¦={sample['confidence']:.3f}")
        if len(wrong_samples) > 5:
            print(f"  ... è¿˜æœ‰ {len(wrong_samples)-5} ä¸ªé”™è¯¯æ ·æœ¬")
    
    # ä¿å­˜ç»“æœ
    summary_result = {
        'platform': 'Python',
        'framework': 'ONNX Runtime Python API',
        'test_type': 'real_mnist_data',
        'data_source': 'MNIST test set subset',
        'results': results,
        'summary': {
            'accuracy': accuracy,
            'average_inference_time_ms': avg_time,
            'std_inference_time_ms': std_time,
            'fps': 1000/avg_time,
            'total_samples': len(results),
            'correct_predictions': correct_predictions,
            'wrong_predictions': len(wrong_samples)
        }
    }
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    os.makedirs('../results', exist_ok=True)
    
    with open('../results/python_inference_mnist_results.json', 'w', encoding='utf-8') as f:
        json.dump(summary_result, f, indent=2, ensure_ascii=False)
    
    print("ç»“æœå·²ä¿å­˜åˆ°: ../results/python_inference_mnist_results.json")
    
    return summary_result

if __name__ == "__main__":
    results = test_python_inference_mnist()
    
    if results:
        print("\nâœ… Pythonæ¨ç†æµ‹è¯•å®Œæˆ")
        print("ç°åœ¨å¯ä»¥è¿è¡ŒC/C++æ¨ç†æµ‹è¯•è¿›è¡Œå¯¹æ¯”")
    else:
        print("âŒ Pythonæ¨ç†æµ‹è¯•å¤±è´¥") 