// C++ ONNXæ¨ç† - ä½¿ç”¨å…±åŒæµ‹è¯•æ•°æ®
#include <iostream>
#include <vector>
#include <chrono>
#include <cmath>
#include <algorithm>
#include <fstream>
#include <iomanip>
#include <stdexcept>
#include <string>

// ä½¿ç”¨C APIé¿å…ä¾èµ–é—®é¢˜
extern "C" {
#include "onnxruntime_c_api.h"
}

// å‰å‘å£°æ˜
class CppONNXInferenceCommon;

// JSONç®€å•è¾“å‡ºç±»
class SimpleJSON {
public:
    template<typename ResultType>
    static void writeInferenceResults(const std::string& filename, 
                                    const std::vector<ResultType>& results,
                                    double avg_time, double accuracy) {
        std::ofstream file(filename);
        if (!file.is_open()) return;
        
        file << "{\n";
        file << "  \"platform\": \"C++\",\n";
        file << "  \"framework\": \"ONNX Runtime C++ API\",\n";
        file << "  \"test_type\": \"common_data\",\n";
        file << "  \"summary\": {\n";
        file << "    \"accuracy\": " << std::fixed << std::setprecision(4) << accuracy << ",\n";
        file << "    \"average_inference_time_ms\": " << std::setprecision(2) << avg_time << ",\n";
        file << "    \"fps\": " << std::setprecision(1) << (1000.0 / avg_time) << ",\n";
        file << "    \"total_samples\": " << results.size() << ",\n";
        
        int correct = 0;
        for (const auto& r : results) {
            if (r.is_correct) correct++;
        }
        file << "    \"correct_predictions\": " << correct << "\n";
        file << "  },\n";
        file << "  \"results\": [\n";
        
        for (size_t i = 0; i < results.size(); ++i) {
            file << "    {\n";
            file << "      \"sample_id\": " << i << ",\n";
            file << "      \"true_label\": " << results[i].true_label << ",\n";
            file << "      \"predicted_class\": " << results[i].predicted_class << ",\n";
            file << "      \"confidence\": " << std::setprecision(4) << results[i].confidence << ",\n";
            file << "      \"inference_time_ms\": " << std::setprecision(2) << results[i].inference_time_ms << ",\n";
            file << "      \"is_correct\": " << (results[i].is_correct ? "true" : "false") << "\n";
            file << "    }";
            if (i < results.size() - 1) file << ",";
            file << "\n";
        }
        
        file << "  ]\n";
        file << "}\n";
        file.close();
    }
};

class CppONNXInferenceCommon {
private:
    const OrtApi* ort_api;
    OrtEnv* env;
    OrtSession* session;
    OrtMemoryInfo* memory_info;
    
public:
    struct InferenceResult {
        int sample_id;
        int true_label;
        int predicted_class;
        float confidence;
        std::vector<float> probabilities;
        double inference_time_ms;
        bool is_correct;
    };
    
    CppONNXInferenceCommon(const std::string& model_path) {
        std::cout << "=== C++ ONNXæ¨ç†æµ‹è¯• (å…±åŒæ•°æ®) ===" << std::endl;
        std::cout << "åˆå§‹åŒ–ONNX Runtime C API..." << std::endl;
        
        // è·å–API
        ort_api = OrtGetApiBase()->GetApi(ORT_API_VERSION);
        
        // åˆ›å»ºç¯å¢ƒ
        OrtStatus* status = ort_api->CreateEnv(ORT_LOGGING_LEVEL_WARNING, "CppONNXInferenceCommon", &env);
        if (status != nullptr) {
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("åˆ›å»ºç¯å¢ƒå¤±è´¥");
        }
        
        // åˆ›å»ºä¼šè¯é€‰é¡¹
        OrtSessionOptions* session_options;
        status = ort_api->CreateSessionOptions(&session_options);
        if (status != nullptr) {
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("åˆ›å»ºä¼šè¯é€‰é¡¹å¤±è´¥");
        }
        
        // åˆ›å»ºä¼šè¯
        status = ort_api->CreateSession(env, model_path.c_str(), session_options, &session);
        if (status != nullptr) {
            ort_api->ReleaseSessionOptions(session_options);
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("åŠ è½½æ¨¡å‹å¤±è´¥: " + model_path);
        }
        
        // åˆ›å»ºå†…å­˜ä¿¡æ¯
        status = ort_api->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info);
        if (status != nullptr) {
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("åˆ›å»ºå†…å­˜ä¿¡æ¯å¤±è´¥");
        }
        
        ort_api->ReleaseSessionOptions(session_options);
        std::cout << "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: " << model_path << std::endl;
    }
    
    ~CppONNXInferenceCommon() {
        if (memory_info) ort_api->ReleaseMemoryInfo(memory_info);
        if (session) ort_api->ReleaseSession(session);
        if (env) ort_api->ReleaseEnv(env);
    }
    
    std::vector<float> preprocess(const std::vector<float>& raw_data) {
        std::vector<float> processed = raw_data;
        
        // MNISTæ ‡å‡†åŒ–
        const float mean = 0.1307f;
        const float std = 0.3081f;
        
        for (auto& pixel : processed) {
            pixel = (pixel - mean) / std;
        }
        
        return processed;
    }
    
    std::vector<float> softmax(const std::vector<float>& logits) {
        std::vector<float> probabilities(logits.size());
        float max_logit = *std::max_element(logits.begin(), logits.end());
        
        float sum = 0.0f;
        for (size_t i = 0; i < logits.size(); ++i) {
            probabilities[i] = std::exp(logits[i] - max_logit);
            sum += probabilities[i];
        }
        
        for (auto& prob : probabilities) {
            prob /= sum;
        }
        
        return probabilities;
    }
    
    InferenceResult inference(int sample_id, int true_label, const std::vector<float>& input_data) {
        auto start = std::chrono::high_resolution_clock::now();
        
        // é¢„å¤„ç†
        auto processed_data = preprocess(input_data);
        
        // åˆ›å»ºè¾“å…¥tensor
        int64_t input_shape[] = {1, 1, 28, 28};
        OrtValue* input_tensor = nullptr;
        
        OrtStatus* status = ort_api->CreateTensorWithDataAsOrtValue(
            memory_info,
            processed_data.data(),
            processed_data.size() * sizeof(float),
            input_shape,
            4,
            ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,
            &input_tensor
        );
        
        if (status != nullptr) {
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("åˆ›å»ºè¾“å…¥tensorå¤±è´¥");
        }
        
        // è®¾ç½®è¾“å…¥è¾“å‡ºåç§°
        const char* input_names[] = {"input"};
        const char* output_names[] = {"output"};
        
        // è¿è¡Œæ¨ç†
        OrtValue* output_tensor = nullptr;
        status = ort_api->Run(
            session,
            nullptr,
            input_names,
            (const OrtValue* const*)&input_tensor,
            1,
            output_names,
            1,
            &output_tensor
        );
        
        if (status != nullptr) {
            ort_api->ReleaseValue(input_tensor);
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("æ¨ç†æ‰§è¡Œå¤±è´¥");
        }
        
        // è·å–è¾“å‡ºæ•°æ®
        float* output_data;
        status = ort_api->GetTensorMutableData(output_tensor, (void**)&output_data);
        if (status != nullptr) {
            ort_api->ReleaseValue(input_tensor);
            ort_api->ReleaseValue(output_tensor);
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("è·å–è¾“å‡ºæ•°æ®å¤±è´¥");
        }
        
        // è·å–è¾“å‡ºå½¢çŠ¶
        OrtTensorTypeAndShapeInfo* output_info;
        status = ort_api->GetTensorTypeAndShape(output_tensor, &output_info);
        if (status != nullptr) {
            ort_api->ReleaseValue(input_tensor);
            ort_api->ReleaseValue(output_tensor);
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("è·å–è¾“å‡ºå½¢çŠ¶å¤±è´¥");
        }
        
        size_t output_count;
        status = ort_api->GetTensorShapeElementCount(output_info, &output_count);
        if (status != nullptr) {
            ort_api->ReleaseValue(input_tensor);
            ort_api->ReleaseValue(output_tensor);
            ort_api->ReleaseTensorTypeAndShapeInfo(output_info);
            ort_api->ReleaseStatus(status);
            throw std::runtime_error("è·å–è¾“å‡ºå…ƒç´ æ•°é‡å¤±è´¥");
        }
        
        // å¤åˆ¶è¾“å‡ºæ•°æ®
        std::vector<float> logits(output_data, output_data + output_count);
        std::vector<float> probabilities = softmax(logits);
        
        // æ‰¾åˆ°é¢„æµ‹ç±»åˆ«
        auto max_it = std::max_element(probabilities.begin(), probabilities.end());
        int predicted_class = std::distance(probabilities.begin(), max_it);
        float confidence = *max_it;
        
        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
        
        // æ¸…ç†èµ„æº
        ort_api->ReleaseValue(input_tensor);
        ort_api->ReleaseValue(output_tensor);
        ort_api->ReleaseTensorTypeAndShapeInfo(output_info);
        
        InferenceResult result;
        result.sample_id = sample_id;
        result.true_label = true_label;
        result.predicted_class = predicted_class;
        result.confidence = confidence;
        result.probabilities = probabilities;
        result.inference_time_ms = duration.count() / 1000.0;
        result.is_correct = (predicted_class == true_label);
        
        return result;
    }
};

// è¯»å–æµ‹è¯•æ•°æ®
std::vector<std::vector<float>> load_test_images(std::vector<int>& true_labels) {
    std::cout << "ğŸ” åŠ è½½å…±åŒæµ‹è¯•æ•°æ®..." << std::endl;
    
    std::vector<std::vector<float>> test_images;
    
    // è¯»å–å…ƒæ•°æ®ä»¥è·å–æ ‡ç­¾ä¿¡æ¯
    std::ifstream metadata_file("../../test_data/metadata.json");
    if (!metadata_file.is_open()) {
        std::cout << "âŒ æ— æ³•æ‰“å¼€å…ƒæ•°æ®æ–‡ä»¶" << std::endl;
        return test_images;
    }
    
    // ç®€å•è§£æJSONè·å–æ ‡ç­¾ï¼ˆè¿™é‡Œç”¨ç®€å•æ–¹æ³•ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥ç”¨JSONåº“ï¼‰
    std::string line;
    std::vector<int> labels;
    while (std::getline(metadata_file, line)) {
        size_t pos = line.find("\"true_label\":");
        if (pos != std::string::npos) {
            pos += 13; // è·³è¿‡ "true_label":
            while (pos < line.length() && (line[pos] == ' ' || line[pos] == '"')) pos++;
            int label = std::stoi(line.substr(pos, 1));
            labels.push_back(label);
        }
    }
    metadata_file.close();
    
    // è¯»å–äºŒè¿›åˆ¶å›¾åƒæ–‡ä»¶
    for (int i = 0; i < 10; ++i) {
        std::string filename = std::string("../../test_data/sample_") + 
                              (i < 10 ? "0" : "") + std::to_string(i) + ".bin";
        
        std::ifstream file(filename, std::ios::binary);
        if (!file.is_open()) {
            std::cout << "âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶: " << filename << std::endl;
            continue;
        }
        
        // è¯»å–28*28ä¸ªfloat32å€¼
        std::vector<float> image_data(28 * 28);
        file.read(reinterpret_cast<char*>(image_data.data()), 28 * 28 * sizeof(float));
        file.close();
        
        test_images.push_back(image_data);
        true_labels.push_back(labels[i]);
        
        std::cout << "æ ·æœ¬ " << i << ": çœŸå®æ ‡ç­¾=" << labels[i] << std::endl;
    }
    
    std::cout << "âœ… åŠ è½½äº† " << test_images.size() << " ä¸ªæµ‹è¯•æ ·æœ¬" << std::endl;
    return test_images;
}

int main() {
    try {
        // åˆå§‹åŒ–æ¨ç†å¼•æ“
        CppONNXInferenceCommon inference_engine("../../models/mnist_model.onnx");
        
        // åŠ è½½æµ‹è¯•æ•°æ®
        std::vector<int> true_labels;
        auto test_images = load_test_images(true_labels);
        
        if (test_images.empty()) {
            std::cout << "âŒ æ²¡æœ‰åŠ è½½åˆ°æµ‹è¯•æ•°æ®" << std::endl;
            return -1;
        }
        
        std::cout << "\nå¼€å§‹æ¨ç† " << test_images.size() << " ä¸ªæ ·æœ¬..." << std::endl;
        
        // æ‰§è¡Œæ¨ç†
        std::vector<CppONNXInferenceCommon::InferenceResult> results;
        double total_time = 0.0;
        int correct_predictions = 0;
        
        for (size_t i = 0; i < test_images.size(); ++i) {
            auto result = inference_engine.inference(i, true_labels[i], test_images[i]);
            results.push_back(result);
            
            total_time += result.inference_time_ms;
            if (result.is_correct) {
                correct_predictions++;
            }
            
            std::cout << "æ ·æœ¬ " << std::setw(2) << i 
                      << ": çœŸå®=" << result.true_label
                      << ", é¢„æµ‹=" << result.predicted_class
                      << ", ç½®ä¿¡åº¦=" << std::fixed << std::setprecision(4) << result.confidence
                      << ", æ—¶é—´=" << std::setprecision(2) << result.inference_time_ms << "ms"
                      << ", " << (result.is_correct ? "âœ“" : "âœ—")
                      << std::endl;
        }
        
        // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        double avg_time = total_time / test_images.size();
        double accuracy = static_cast<double>(correct_predictions) / test_images.size();
        
        std::cout << "\n=== æ¨ç†ç»“æœç»Ÿè®¡ ===" << std::endl;
        std::cout << "æ€»æ ·æœ¬æ•°: " << test_images.size() << std::endl;
        std::cout << "æ­£ç¡®é¢„æµ‹: " << correct_predictions << std::endl;
        std::cout << "å‡†ç¡®ç‡: " << std::fixed << std::setprecision(2) << accuracy * 100 << "%" << std::endl;
        std::cout << "å¹³å‡æ¨ç†æ—¶é—´: " << std::setprecision(2) << avg_time << "ms" << std::endl;
        std::cout << "æ¨ç†é€Ÿåº¦: " << std::setprecision(1) << 1000.0 / avg_time << " FPS" << std::endl;
        
        // ä¿å­˜ç»“æœ
        system("mkdir -p ../../results 2>/dev/null || mkdir ..\\..\\results 2>nul || true");
        SimpleJSON::writeInferenceResults("../../results/cpp_inference_common_results.json", 
                                        results, avg_time, accuracy);
        std::cout << "ç»“æœå·²ä¿å­˜åˆ°: ../../results/cpp_inference_common_results.json" << std::endl;
        
        std::cout << "\nâœ… C++æ¨ç†æµ‹è¯•å®Œæˆï¼" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ é”™è¯¯: " << e.what() << std::endl;
        return -1;
    }
    
    return 0;
} 